{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dfc4084a",
   "metadata": {},
   "source": [
    "## <font color='#0000FF'>MSC_DA_CA2 - INTEGRATEDCA2<font color='#1ABC9D'>\n",
    "### <font color='#'>**Advanced Data Analytics  & Big Data Storage and Processing**\n",
    "### <font color='#1ABC9C'>**Lecturer(s): David McQuaid and Muhammad Iqbal**\n",
    "------\n",
    "<font color='#1ABC9C'>**Student name / ID** // Rosilene Francisca da Silva - 2021090"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ad9063",
   "metadata": {},
   "source": [
    "### Data Integration and Preprocessing: Leveraging Apache Spark for Populating MySQL Databases with Large Datasets.\n",
    "\n",
    "### Configure Apache Spark\n",
    "Start Spark Session:\n",
    "Set up PySpark, including necessary packages for MySQL connections."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c80ae7b",
   "metadata": {},
   "source": [
    "\"The dataset used in this analysis, ProjectTweets.csv, was provided by Professor [McQuaid] via the Moodle [https://moodle.cct.ie/mod/assign/view.php?id=44089]course [MSc in Data Analytics] CCT College Dublin on 18 April 2024.\"\n",
    "\n",
    "The dataset is guaranteed to have an organised schema, dependable transactional integrity, strong query capabilities, and simple integration with Apache Spark by selecting MySQL over a NoSQL database. All of these advantages combine to make MySQL the superior option for this particular task of populating the information and doing further analytical processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c3df83",
   "metadata": {},
   "outputs": [],
   "source": [
    "Don't run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c121d9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c45c7de2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/11 19:12:25 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "24/05/11 19:12:25 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf, SparkContext\n",
    "import os\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Database Comparative Analysis\") \\\n",
    "    .config(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\") \\\n",
    "    .config(\"spark.driver.memory\", \"8g\") \\\n",
    "    .config(\"spark.executor.memory\", \"8g\") \\\n",
    "    .config(\"spark.executor.cores\", \"4\") \\\n",
    "    .config(\"spark.dynamicAllocation.enabled\", \"true\") \\\n",
    "    .config(\"spark.dynamicAllocation.minExecutors\", \"2\") \\\n",
    "    .config(\"spark.dynamicAllocation.maxExecutors\", \"100\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"1000\") \\\n",
    "    .config(\"spark.jars\", \"/home/hduser/Downloads/mysql-connector-j-8.0.33.jar\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1cb9ec26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.0.2.15:4042\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.4.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Database Comparative Analysis</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x732653fa65c0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17c0369c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10.9.7\n"
     ]
    }
   ],
   "source": [
    "import py4j\n",
    "print(py4j.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24367509",
   "metadata": {},
   "source": [
    "Rationale:\n",
    "This PySpark setup is designed to process data during a database comparison in an effective and scalable manner. With the application name \"Database Comparative Analysis,\" the SparkSession is created, and backward compatibility with timestamp formats is guaranteed by the legacy time parser policy. In order to efficiently handle huge data volumes, the memory configurations for the executors and driver are set to 8 GB each. Moreover, having 4 cores per executor optimises parallel processing.\n",
    "\n",
    "By dynamically allocating resources according to workload requirements, the cluster can grow from a minimum of 2 executors to a maximum of 100. Large dataset data shuffling speed is enhanced by increasing the shuffle divisions to 1000, which reduces execution time. \n",
    "\n",
    "Furthermore, smooth connectivity between Spark and MySQL for direct data intake and analysis is ensured by including the MySQL JDBC connector jar. This setup offers a scalable and highly effective environment appropriate for large-scale database comparison analysis projects."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d2c919",
   "metadata": {},
   "source": [
    "#### Before attempting to read the data, let's test reading data or just establish a connection to identify if has any issue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "38cbc2a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-------------------+------------+-------------+--------------------+\n",
      "| id|   user_id|               date|query_status|  user_handle|          tweet_text|\n",
      "+---+----------+-------------------+------------+-------------+--------------------+\n",
      "|  1|1467810672|2009-04-06 22:19:49|    NO_QUERY|scotthamilton|is upset that he ...|\n",
      "+---+----------+-------------------+------------+-------------+--------------------+\n",
      "only showing top 1 row\n",
      "\n",
      "Connection established successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Test loading a simple query to ensure connectivity\n",
    "try:\n",
    "    jdbc_df = spark.read.format(\"jdbc\").option(\"url\", \"jdbc:mysql://localhost/twitter_data\").option(\n",
    "        \"driver\", \"com.mysql.cj.jdbc.Driver\").option(\"dbtable\", \"tweet_details\").option(\n",
    "        \"user\", \"root\").option(\"password\", \"password\").load()\n",
    "    jdbc_df.show(1)\n",
    "    print(\"Connection established successfully.\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(\"An error occurred:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bfa16e9",
   "metadata": {},
   "source": [
    "### Load the data from MySQL into a new DataFrame\n",
    "The Database `twitter_data` was created on MySQL following the `tweet_details`table.  \n",
    "Also defined and renamed the column names as \"id\", \"user_id\", \"date\", \"query_status\", \"user_handle\", and \"tweet_text\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2774ce37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-------------------+------------+-------------+--------------------+\n",
      "| id|   user_id|               date|query_status|  user_handle|          tweet_text|\n",
      "+---+----------+-------------------+------------+-------------+--------------------+\n",
      "|  1|1467810672|2009-04-06 22:19:49|    NO_QUERY|scotthamilton|is upset that he ...|\n",
      "|  2|1467810917|2009-04-06 22:19:53|    NO_QUERY|     mattycus|@Kenichan I dived...|\n",
      "+---+----------+-------------------+------------+-------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Load the data from MySQL into a new DataFrame\n",
    "mysql_df = spark.read.format(\"jdbc\").options(\n",
    "    url=\"jdbc:mysql://localhost/twitter_data\",\n",
    "    driver=\"com.mysql.cj.jdbc.Driver\",\n",
    "    dbtable=\"tweet_details\",\n",
    "    user=\"root\",\n",
    "    password=\"password\"\n",
    ").load()\n",
    "\n",
    "# Show the first few rows of the DataFrame to verify the data\n",
    "mysql_df.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ac2d93",
   "metadata": {},
   "source": [
    "This code loaded the data from the tweet_details table in MySQL into a new DataFrame called mysql_df. As the data was displayed correctly, it indicates that the data was successfully read to MySQL from PySpark."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c390b498",
   "metadata": {},
   "source": [
    "#### Verify the DataFrame\n",
    "After load the dataset, it’s a good idea to check the type and first few rows of the DataFrame to ensure that everything is loaded correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "91605414",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- user_id: long (nullable = true)\n",
      " |-- date: timestamp (nullable = true)\n",
      " |-- query_status: string (nullable = true)\n",
      " |-- user_handle: string (nullable = true)\n",
      " |-- tweet_text: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-------------------+------------+-------------+--------------------+\n",
      "| id|   user_id|               date|query_status|  user_handle|          tweet_text|\n",
      "+---+----------+-------------------+------------+-------------+--------------------+\n",
      "|  1|1467810672|2009-04-06 22:19:49|    NO_QUERY|scotthamilton|is upset that he ...|\n",
      "|  2|1467810917|2009-04-06 22:19:53|    NO_QUERY|     mattycus|@Kenichan I dived...|\n",
      "|  3|1467811184|2009-04-06 22:19:57|    NO_QUERY|      ElleCTF|my whole body fee...|\n",
      "|  4|1467811193|2009-04-06 22:19:57|    NO_QUERY|       Karoli|@nationwideclass ...|\n",
      "|  5|1467811372|2009-04-06 22:20:00|    NO_QUERY|     joy_wolf|@Kwesidei not the...|\n",
      "+---+----------+-------------------+------------+-------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Print DataFrame schema\n",
    "mysql_df.printSchema()\n",
    "\n",
    "mysql_df.show(5, truncate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba690cd",
   "metadata": {},
   "source": [
    "#### Dataframe Information\n",
    "Show the number of rows and columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f3050bad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 1599999, Number of columns: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "num_rows = mysql_df.count()\n",
    "num_columns = len(mysql_df.columns)\n",
    "print(f\"Number of rows: {num_rows}, Number of columns: {num_columns}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545bb96e",
   "metadata": {},
   "source": [
    "With 1,599,999 rows and 7 columns overall, the dataset has a sizable amount of data that may be thoroughly examined. Each row contains particular information possibly connected to tweets, with seven columns reflecting different dataset aspects. These features include numerical and categorical data. This dataset's extensive structure makes it possible to perform significant exploratory data analysis (EDA) and gain important insights about user behaviour and data patterns. It also makes it possible to perform more complex analysis or machine learning activities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618888f1",
   "metadata": {},
   "source": [
    "### Data Pre-Processing in PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "518a2ddd",
   "metadata": {},
   "source": [
    "#### Checking for missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "726d51d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 21:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No missing values found in any column.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Initialize a flag to track whether missing values are found\n",
    "from pyspark.sql.functions import col\n",
    "missing_values_found = False\n",
    "\n",
    "for column in mysql_df.columns:\n",
    "    missing_count = mysql_df.filter(col(column).isNull() | (col(column) == '')).count()\n",
    "    if missing_count > 0:\n",
    "        print(f\"Column {column} has {missing_count} missing values\")\n",
    "        missing_values_found = True\n",
    "\n",
    "# Check the flag after checking all columns, and print a message if no missing values were found\n",
    "if not missing_values_found:\n",
    "    print(\"No missing values found in any column.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c76c77",
   "metadata": {},
   "source": [
    "Based on output the dataset there is no missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "81b9be9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('id', 'int'),\n",
       " ('user_id', 'bigint'),\n",
       " ('date', 'timestamp'),\n",
       " ('query_status', 'string'),\n",
       " ('user_handle', 'string'),\n",
       " ('tweet_text', 'string')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Displaying dtypes of columns\n",
    "mysql_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f01061",
   "metadata": {},
   "source": [
    "From the output the DataFrame schema, the columns `id` and `user_id` are numerical. \n",
    "The column `date`  has a type of timestamp, meaning it's a timestamp (or datetime) field, columns like `query_status` `user_handle`, and `tweet_text` has a type of string, meaning it's a textual field (or varchar in SQL)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bcf616f",
   "metadata": {},
   "source": [
    "#### Displaying some columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f7772b7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 24:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|user_id   |\n",
      "+----------+\n",
      "|1467810672|\n",
      "|1467810917|\n",
      "|1467811184|\n",
      "|1467811193|\n",
      "|1467811372|\n",
      "+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "mysql_df.select('user_id').show(n=5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8d3e4121",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|date               |\n",
      "+-------------------+\n",
      "|2009-04-06 22:19:49|\n",
      "|2009-04-06 22:19:53|\n",
      "|2009-04-06 22:19:57|\n",
      "|2009-04-06 22:19:57|\n",
      "|2009-04-06 22:20:00|\n",
      "+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mysql_df.select('date').show(n=5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f174f8ba",
   "metadata": {},
   "source": [
    "The output illustrates the temporal character of the data by showing the first five rows of the date column from the dataset. Each entry in the date column is a timestamp indicating when a particular event, such as a tweet, occurred."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575ad16a",
   "metadata": {},
   "source": [
    "#### Using the.agg() method and the aggregation functions min and max to find the earliest and latest dates will print the first and last tweet dates. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "74f2f276",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 26:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First tweet date: 2009-04-06 22:19:49\n",
      "Last tweet date: 2009-06-25 10:28:31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Import Aggregation Functions\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Finding the first and last tweet dates\n",
    "first_last_dates = mysql_df.agg(\n",
    "    F.min(\"date\").alias(\"first_date\"),\n",
    "    F.max(\"date\").alias(\"last_date\")\n",
    ").collect()[0]\n",
    "\n",
    "first_date = first_last_dates[\"first_date\"]\n",
    "last_date = first_last_dates[\"last_date\"]\n",
    "\n",
    "print(f\"First tweet date: {first_date}\")\n",
    "print(f\"Last tweet date: {last_date}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a0ec4c",
   "metadata": {},
   "source": [
    "The earliest and latest timestamps for tweets suggest that the dataset covers the period of April 6, 2009, to June 25, 2009. The last tweet was sent out on June 25, 2009, at 10:28:31, and the first one was sent out on April 6, 2009, at 22:19:49. \n",
    "\n",
    "According to this, the dataset includes tweets from over three months' worth of time, giving it a temporal range that is appropriate for studying tweet patterns and trends during that time. The outcome helps to clarify the historical bounds of the dataset and directs further data analysis techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4271c440",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 29:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------------------------------------------------------------------------------------------------------------+\n",
      "|user_handle  |tweet_text                                                                                                     |\n",
      "+-------------+---------------------------------------------------------------------------------------------------------------+\n",
      "|scotthamilton|is upset that he can't update his Facebook by texting it... and might cry as a result  School today also. Blah!|\n",
      "|mattycus     |@Kenichan I dived many times for the ball. Managed to save 50%  The rest go out of bounds                      |\n",
      "|ElleCTF      |my whole body feels itchy and like its on fire                                                                 |\n",
      "|Karoli       |@nationwideclass no, it's not behaving at all. i'm mad. why am i here? because I can't see you all over there. |\n",
      "|joy_wolf     |@Kwesidei not the whole crew                                                                                   |\n",
      "+-------------+---------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Selecting multiple columns\n",
    "mysql_df.select(['user_handle','tweet_text']).show(n=5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87fceeec",
   "metadata": {},
   "source": [
    "The dataset's first rows provide insight into the `user_handle` and `tweet_text` columns. The user_handle column contains Twitter handles that indicate who wrote each tweet, but the tweet_text column has the actual text content of each tweet. For example, user scotthamilton complains about not being able to update Facebook via text, whereas mattycus describes diving for a ball. Other tweets show personal discomfort (ElleCTF), bewilderment (Karoli), and interactions with other users (joy_wolf). This selection captures various user moods and activities on Twitter within the dataset's duration, emphasising the diversity of content, which ranges from personal updates to social interactions. The truncate=False argument guarantees that the complete content of each tweet is displayed without truncation, providing a preview of the user's tweet interactions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa94228",
   "metadata": {},
   "source": [
    "#### Data Distribution\n",
    "Count distinct values of a categorical column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7e4bda44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 36:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------+\n",
      "|query_status|  count|\n",
      "+------------+-------+\n",
      "|    NO_QUERY|1599999|\n",
      "+------------+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Selecting 'query_status' column\n",
    "mysql_df.groupBy(\"query_status\").count().show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa00cb9",
   "metadata": {},
   "source": [
    "The dataset's `query_status` column. In this instance, the output indicates that the query_status of NO_QUERY applies to all 1,599,999 tweets. This suggests that the query_status column has a consistent value across all rows, indicating that the field was not meaningfully used in the dataset or that none of the tweets were connected to any particular search query. The consistency shows that, absent other columns or contextual information, the column might not be pertinent for more investigation. All things considered, this discovery can direct data analysts to concentrate on other columns for significant insights while taking into account possible drop this column from further studies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "734dd5cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 42:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----+\n",
      "|    user_handle|count|\n",
      "+---------------+-----+\n",
      "|     megan_rice|   15|\n",
      "|         MeghTW|    1|\n",
      "|stranger_danger|   14|\n",
      "|       kyrabeth|    1|\n",
      "|    lovelylivxo|   16|\n",
      "|      tink68113|    1|\n",
      "|     Svalentyna|    1|\n",
      "|     bakerbelle|    1|\n",
      "|  somethingalex|    1|\n",
      "|     sexy_ass_T|    1|\n",
      "+---------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Selecting 'user_handle' column\n",
    "mysql_df.groupBy(\"user_handle\").count().show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a08493",
   "metadata": {},
   "source": [
    "The tweet count of distinct users is calculated by grouping the dataset by the `user_handle` column and counting the number of tweets linked with each user. The first ten rows of the output reveal a unique Twitter user and the number of tweets they have sent. For example, megan_rice has 15 tweets, stranger_danger has 14, and lovelylivxo has 16, yet MeghTW, kyrabeth, so on each have only one tweet. This distribution shows that, while some individuals are frequent tweeters, others have a low presence in the dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27021cd2",
   "metadata": {},
   "source": [
    "#### Summary Statistics\n",
    "Generate summary statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "96573828",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/11 19:16:41 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "[Stage 45:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+--------------------+------------+--------------------+--------------------+\n",
      "|summary|               id|             user_id|query_status|         user_handle|          tweet_text|\n",
      "+-------+-----------------+--------------------+------------+--------------------+--------------------+\n",
      "|  count|          1599999|             1599999|     1599999|             1599999|             1599999|\n",
      "|   mean|         800000.0|1.9988178841753244E9|        null| 4.325887521835714E9|                null|\n",
      "| stddev|461880.0710141109| 1.935756789172917E8|        null|5.162733218454889E10|                null|\n",
      "|    min|                1|          1467810672|    NO_QUERY|        000catnap000|                 ...|\n",
      "|    max|          1599999|          2329205794|    NO_QUERY|          zzzzeus111|ï¿½ï¿½ï¿½ï¿½ï¿½ß§...|\n",
      "+-------+-----------------+--------------------+------------+--------------------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Basic statistics\n",
    "mysql_df.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09029b04",
   "metadata": {},
   "source": [
    "The gives summary statistics for the dataset, shedding light on the distribution and features of each column. \n",
    "\n",
    "With a mean of 800,000.0 and a standard deviation of roughly 461,880, the id column shows a well-distributed set of unique identifiers ranging from 1 to 1,599,999. With a mean of almost 1.998 billion, the user_id column, which represents unique user identifiers, ranges from 1,467,810,672 to 2,329,205,794. All rows in the query_status column have the same value, NO_QUERY. The Twitter `user_handle` column span from 000catnap000 to zzzzeus111, indicating a wide range of users. \n",
    "Finally, a variety of text data are contained in the tweet_text column, including special characters that are evident in the maximum value ï¿½ï¿½ï¿½ï¿½ß§..., which may be indicative of encoding problems. A glimpse of the data distribution is given by the summary, which shows that although most columns are diverse and well-populated, the `query_status` column is uniform, and special characters would need to be cleaned up."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6d283b",
   "metadata": {},
   "source": [
    "#### Correlation Analysis\n",
    "Find correlations between numerical columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bd77775d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numerical columns: ['id', 'user_id']\n"
     ]
    }
   ],
   "source": [
    "# List of numerical columns\n",
    "numerical_columns = [col for col, dtype in mysql_df.dtypes if dtype in ('int', 'bigint', 'double', 'float')]\n",
    "print(f\"Numerical columns: {numerical_columns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e0bdc852",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 48:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+--------------------+\n",
      "|summary|               id|             user_id|\n",
      "+-------+-----------------+--------------------+\n",
      "|  count|          1599999|             1599999|\n",
      "|   mean|         800000.0|1.9988178841753244E9|\n",
      "| stddev|461880.0710141109| 1.935756789172917E8|\n",
      "|    min|                1|          1467810672|\n",
      "|    max|          1599999|          2329205794|\n",
      "+-------+-----------------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Summary statistics only for numerical columns\n",
    "mysql_df.select(numerical_columns).describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "904061ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation between id and user_id: 0.22304937463402058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 54:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation between user_id and id: 0.22304937463402053\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Calculate correlations between numerical columns\n",
    "from pyspark.sql.functions import corr\n",
    "\n",
    "for column1 in numerical_columns:\n",
    "    for column2 in numerical_columns:\n",
    "        if column1 != column2:\n",
    "            correlation = mysql_df.select(corr(column1, column2)).first()[0]\n",
    "            print(f\"Correlation between {column1} and {column2}: {correlation}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e9c51b",
   "metadata": {},
   "source": [
    "There is roughly a 0.223 correlation between `id` and `user_id`. Although there is a linear link between the two columns, it is not very strong, as indicated by the slight positive correlation between id and user_id. This discovery may help direct future research or data modelling by providing insight into the distribution of data and user behaviour."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff64bf48",
   "metadata": {},
   "source": [
    "#### Adding a New Column with the current timestamp & Extract only those tweets that contain the keyword \"summer\" in the tweet_text column.\n",
    "The `inserted_at` column will contain the timestamp indicating when each row was processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cfd07660",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 57:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+--------------------+\n",
      "|               date|          tweet_text|         inserted_at|\n",
      "+-------------------+--------------------+--------------------+\n",
      "|2009-04-06 22:27:00|@jacobsummers Sor...|2024-05-11 19:17:...|\n",
      "|2009-04-06 23:40:34|@kimmyawesome Ohh...|2024-05-11 19:17:...|\n",
      "|2009-04-06 23:48:30|It's official! I'...|2024-05-11 19:17:...|\n",
      "|2009-04-07 00:12:36|ok my TWEET PEEP ...|2024-05-11 19:17:...|\n",
      "|2009-04-07 00:13:15|summer camp or su...|2024-05-11 19:17:...|\n",
      "|2009-04-07 00:34:43|Downy weather  Wh...|2024-05-11 19:17:...|\n",
      "|2009-04-07 00:35:12|Craaaaap. My Macb...|2024-05-11 19:17:...|\n",
      "|2009-04-07 00:41:46|@dadi_iyal and yo...|2024-05-11 19:17:...|\n",
      "|2009-04-07 01:57:26|@meatrack no more...|2024-05-11 19:17:...|\n",
      "|2009-04-07 01:57:35|searching for a j...|2024-05-11 19:17:...|\n",
      "+-------------------+--------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Filtering tweets containing a specific keyword 'summer'\n",
    "filtered_df = mysql_df.filter(mysql_df.tweet_text.contains(\"summer\"))\n",
    "\n",
    "# Add a current timestamp column for the insert time\n",
    "from pyspark.sql.functions import current_timestamp\n",
    "\n",
    "final_df = filtered_df.withColumn(\"inserted_at\", current_timestamp())\n",
    "\n",
    "# Select only the desired columns and print it\n",
    "final_df.select(\"date\", \"tweet_text\", \"inserted_at\").show(10, truncate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e161afb1",
   "metadata": {},
   "source": [
    "After filtering the dataset, this result displays the first ten rows of tweets that contain the keyword \"summer\". Each tweet is shown with its original timestamp (date), the tweet's text (tweet_text), and a new column (inserted_at) that shows the current timestamp when this information was processed. The date column providing information on the temporal distribution of tweets mentioning \"summer.\" For example, one user is undecided between \"summer camp or summer school,\" while another tweets about \"Downy Weather.\" The `inserted_at` column indicates that the data was processed, distinguishing between the original tweet timestamps and the time of processing. \n",
    "\n",
    "This allows for the tracking of when the filtered data was curated. Overall, this result displays a wide range of attitudes and interests relating to \"summer,\" from plans to technical concerns, providing useful insights into user interactions about this topic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad0ca1a",
   "metadata": {},
   "source": [
    "#### Word Frequency Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b1086f72",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 60:>                                                         (0 + 4) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "|word|  count|\n",
      "+----+-------+\n",
      "|    |1184159|\n",
      "|  to| 552961|\n",
      "|   I| 496616|\n",
      "| the| 487501|\n",
      "|   a| 366211|\n",
      "|  my| 280025|\n",
      "| and| 275263|\n",
      "|   i| 249976|\n",
      "|  is| 217692|\n",
      "| you| 213871|\n",
      "+----+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Frequency of words in the \"tweet_text\" column\n",
    "from pyspark.sql.functions import explode, split\n",
    "mysql_df.withColumn(\"word\", explode(split(mysql_df[\"tweet_text\"], \"\\s+\"))).groupBy(\"word\").count().orderBy(\"count\", ascending=False).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a47464",
   "metadata": {},
   "source": [
    "To analyze text data, paying particular attention to word frequency in the \"tweet_text\" column. The DataFrame is then grouped according to the \"word\" column, and the count() method is used to determine how many times each word appears. The most common terms are then highlighted by sorting the results in descending order using the \"count\" column. Lastly, the top ten outcomes are shown. The output displays the word counts. Common English terms like \"to,\" \"I,\" and \"the\" are followed by the most often occurring word, 1,184,159 times (blank spaces, probably from many consecutive spaces or formatting errors in tweets).\n",
    "\n",
    "Rationale: Understanding the dataset's typical word usage, identifying any data cleaning problems (such as the large number of blanks), and using the results as a foundation for additional text analysis tasks like sentiment analysis or theme modelling are all made possible by this analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e35521db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['id', 'user_id', 'date', 'query_status', 'user_handle', 'tweet_text']\n"
     ]
    }
   ],
   "source": [
    "# Print current DataFrame columns\n",
    "print(mysql_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "973eb182",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('id', 'int'),\n",
       " ('user_id', 'bigint'),\n",
       " ('date', 'timestamp'),\n",
       " ('query_status', 'string'),\n",
       " ('user_handle', 'string'),\n",
       " ('tweet_text', 'string')]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Displaying dtypes of columns\n",
    "mysql_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8bfdc45",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "### Converting the dataset from MySQL to Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3163a1ce",
   "metadata": {},
   "source": [
    "#### Before converting the dataset to pandas, drop some unnecessary columns for further analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a851d6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Droping'id','query_status' columns\n",
    "mysql_df = mysql_df.drop('id','query_status')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e34ec8a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 61:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+-------------+--------------------+\n",
      "|   user_id|               date|  user_handle|          tweet_text|\n",
      "+----------+-------------------+-------------+--------------------+\n",
      "|1467810672|2009-04-06 22:19:49|scotthamilton|is upset that he ...|\n",
      "|1467810917|2009-04-06 22:19:53|     mattycus|@Kenichan I dived...|\n",
      "+----------+-------------------+-------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "mysql_df.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb86ca02",
   "metadata": {},
   "source": [
    "#### Checking for missing Values again after dropped some columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "74f1e628",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 62:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+-----------+----------+\n",
      "|user_id|date|user_handle|tweet_text|\n",
      "+-------+----+-----------+----------+\n",
      "|      0|   0|          0|         0|\n",
      "+-------+----+-----------+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, sum\n",
    "\n",
    "null_count = mysql_df.select([sum(col(c).isNull().cast(\"int\")).alias(c) for c in mysql_df.columns])\n",
    "null_count.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c191f1",
   "metadata": {},
   "source": [
    "#### Collect the Dataset to Pandas \n",
    "Objective: Gather all rows from the Spark DataFrame (mysql_df) into a list of rows.\n",
    "\n",
    "Functionality:\n",
    ".collect(): Fetches all data rows from the distributed environment into the driver node as a list of Row objects.\n",
    "Rationale: Because this operation requires sufficient memory and is suitable for relatively smaller datasets due to its memory-intensive nature, the memory configurations for the executors and driver were set from 4 GB to 8 GB each, and the configuration bellow on Spark in the beginning was changed, and unnecessary columns were removed before running the process.    \n",
    "    .config(\"spark.executor.cores\", \"4\") \\\n",
    "    .config(\"spark.dynamicAllocation.enabled\", \"true\") \\\n",
    "    .config(\"spark.dynamicAllocation.minExecutors\", \"2\") \\\n",
    "    .config(\"spark.dynamicAllocation.maxExecutors\", \"100\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"1000\") \\ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9280b11b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Collect the Dataset to Pandas\n",
    "new_df = mysql_df.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f055c8e1",
   "metadata": {},
   "source": [
    "#### Create Pandas DataFrame from MySQL\n",
    "Objective: Convert the list of rows into a Pandas DataFrame for further analysis.\n",
    "\n",
    "Functionality: pd.DataFrame(new_df): Initializes a Pandas DataFrame from the list of rows.\n",
    "twitter_df.columns = mysql_df.columns: Assigns the original column names from the Spark DataFrame (mysql_df) to the Pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b5bcdb0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>date</th>\n",
       "      <th>user_handle</th>\n",
       "      <th>tweet_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1467810672</td>\n",
       "      <td>2009-04-06 22:19:49</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1467810917</td>\n",
       "      <td>2009-04-06 22:19:53</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1467811184</td>\n",
       "      <td>2009-04-06 22:19:57</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1467811193</td>\n",
       "      <td>2009-04-06 22:19:57</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1467811372</td>\n",
       "      <td>2009-04-06 22:20:00</td>\n",
       "      <td>joy_wolf</td>\n",
       "      <td>@Kwesidei not the whole crew</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      user_id                date    user_handle  \\\n",
       "0  1467810672 2009-04-06 22:19:49  scotthamilton   \n",
       "1  1467810917 2009-04-06 22:19:53       mattycus   \n",
       "2  1467811184 2009-04-06 22:19:57        ElleCTF   \n",
       "3  1467811193 2009-04-06 22:19:57         Karoli   \n",
       "4  1467811372 2009-04-06 22:20:00       joy_wolf   \n",
       "\n",
       "                                          tweet_text  \n",
       "0  is upset that he can't update his Facebook by ...  \n",
       "1  @Kenichan I dived many times for the ball. Man...  \n",
       "2    my whole body feels itchy and like its on fire   \n",
       "3  @nationwideclass no, it's not behaving at all....  \n",
       "4                      @Kwesidei not the whole crew   "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create Pandas DataFrame from MySQL\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "twitter_df = pd.DataFrame(new_df)\n",
    "twitter_df.columns =  mysql_df.columns\n",
    "twitter_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3d3b98e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1599999, 4)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778d0e50",
   "metadata": {},
   "source": [
    "The techniques was applyed to collect a large dataset from a Spark DataFrame (mysql_df) to a Pandas DataFrame (twitter_df) for further analysis. .collect() gathers all rows from the Spark DataFrame into a list of Row objects, which are then transformed to a Pandas DataFrame. The resulting twitter_df has 1,599,999 rows and 4 columns, indicating that the dataset is large enough to be explored and manipulated with Pandas. However, due to the scale of the dataset, attention should be given with memory utilisation, as Pandas operations might be more memory-intensive than Spark DataFrames."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989a2768",
   "metadata": {},
   "source": [
    "### Save the Dataset to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "97e73e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the Pandas DataFrame to a CSV file\n",
    "twitter_df.to_csv('twitter_df.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c9e9ae85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 401M\r\n",
      "-rw-r--r-- 1 hduser hadoopgroup  53K May 11 19:17 'Integrated_CA2MScDA_ BD_ADA -BDS1.ipynb'\r\n",
      "-rw-r--r-- 1 hduser hadoopgroup 104K May 11 19:18 'Integrated_CA2MScDA_ BD_ADA .ipynb'\r\n",
      "drwxr-xr-x 4 hduser hadoopgroup 4.0K May  6 10:02  MSCCA12023V2\r\n",
      "-rw-r--r-- 1 hduser hadoopgroup 2.4M May  5 15:20  mysql-connector-j-8.0.33.jar\r\n",
      "-rw-r--r-- 1 hduser hadoopgroup 219M Apr 28 09:50  ProjectTweets.csv\r\n",
      "-rw-r--r-- 1 hduser hadoopgroup   32 Apr 25 09:23  README.md\r\n",
      "-rw-r--r-- 1 hduser hadoopgroup 180M May 11 19:20  twitter_df.csv\r\n"
     ]
    }
   ],
   "source": [
    "!ls -lh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e61b14",
   "metadata": {},
   "source": [
    "The collected  DataFrame (twitter_df) is saved to a CSV file named twitter_df.csv via the to_csv() method. The index=False argument ensures that the index column does not appear in the output. This method facilitates data export, allowing the dataset to be shared, analysed, or preserved in CSV format. The generated CSV file has 1,599,999 rows and four columns (user_id, date, user_handle, and tweet_text), offering a complete and portable snapshot of Twitter data for further analysis.\n",
    "\n",
    "The command `!ls -lh` was applyed to list all files in the current directory with detailed information like size, permissions, owner, and timestamps in a readable format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb55a4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce3f96df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f787d3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3caa4a7e",
   "metadata": {},
   "source": [
    "# Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0f02ae83",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Importing the libraries.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "from matplotlib import cm\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2973bced",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Command to display all columns in the file.\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9546bb7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>date</th>\n",
       "      <th>user_handle</th>\n",
       "      <th>tweet_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1467810672</td>\n",
       "      <td>2009-04-06 22:19:49</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1467810917</td>\n",
       "      <td>2009-04-06 22:19:53</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1467811184</td>\n",
       "      <td>2009-04-06 22:19:57</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1467811193</td>\n",
       "      <td>2009-04-06 22:19:57</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1467811372</td>\n",
       "      <td>2009-04-06 22:20:00</td>\n",
       "      <td>joy_wolf</td>\n",
       "      <td>@Kwesidei not the whole crew</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      user_id                 date    user_handle  \\\n",
       "0  1467810672  2009-04-06 22:19:49  scotthamilton   \n",
       "1  1467810917  2009-04-06 22:19:53       mattycus   \n",
       "2  1467811184  2009-04-06 22:19:57        ElleCTF   \n",
       "3  1467811193  2009-04-06 22:19:57         Karoli   \n",
       "4  1467811372  2009-04-06 22:20:00       joy_wolf   \n",
       "\n",
       "                                          tweet_text  \n",
       "0  is upset that he can't update his Facebook by ...  \n",
       "1  @Kenichan I dived many times for the ball. Man...  \n",
       "2    my whole body feels itchy and like its on fire   \n",
       "3  @nationwideclass no, it's not behaving at all....  \n",
       "4                      @Kwesidei not the whole crew   "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing the CSV Dataset \n",
    "twitter_df = pd.read_csv('twitter_df.csv') \n",
    "twitter_df.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "68f03763",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1599999, 4)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "09841a05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1599999 entries, 0 to 1599998\n",
      "Data columns (total 4 columns):\n",
      " #   Column       Non-Null Count    Dtype \n",
      "---  ------       --------------    ----- \n",
      " 0   user_id      1599999 non-null  int64 \n",
      " 1   date         1599999 non-null  object\n",
      " 2   user_handle  1599999 non-null  object\n",
      " 3   tweet_text   1599999 non-null  object\n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 48.8+ MB\n"
     ]
    }
   ],
   "source": [
    "twitter_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1819b85",
   "metadata": {},
   "source": [
    "Based on the first basic details, the dataset appears to be a collection of tweets, commonly can be use in sentiment analysis or other natural language processing tasks. \n",
    "\n",
    "Dataset Size and Integrity:\n",
    "The dataset contains approximately 1.6 million entries (1599999 entries, indexed from 0 to 1599998) and 4 columns.\n",
    "Each column has the same number of non-null entries (1599999), indicating there are no missing values in any of the columns.\n",
    "Memory Usage: The dataframe is consuming approximately 48.8+ MB of memory, which is relevant for understanding the computation resources needed to process this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7935cd07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "user_id         int64\n",
       "date           object\n",
       "user_handle    object\n",
       "tweet_text     object\n",
       "dtype: object"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4577a43c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of data is 1599999\n"
     ]
    }
   ],
   "source": [
    "print('length of data is', len(twitter_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fcbf05b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter_df.isnull().values.any()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91173610",
   "metadata": {},
   "source": [
    "#### Display the Unique Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "594f8194",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique values in 'user_id': 1598314\n",
      "Number of unique values in 'date': 774362\n",
      "Number of unique values in 'user_handle': 659775\n",
      "Number of unique values in 'tweet_text': 1581465\n"
     ]
    }
   ],
   "source": [
    "for column in twitter_df.columns:\n",
    "    num_unique_values = twitter_df[column].nunique()\n",
    "    print(f\"Number of unique values in '{column}': {num_unique_values}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c4786a74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Features  Uniques\n",
      "0      user_id  1598314\n",
      "1         date   774362\n",
      "2  user_handle   659775\n",
      "3   tweet_text  1581465\n"
     ]
    }
   ],
   "source": [
    "def return_unique_values(data_frame):\n",
    "    unique_dataframe = pd.DataFrame()\n",
    "    unique_dataframe['Features'] = data_frame.columns\n",
    "    uniques = []\n",
    "    for col in data_frame.columns:\n",
    "        u = data_frame[col].nunique()\n",
    "        uniques.append(u)\n",
    "    unique_dataframe['Uniques'] = uniques\n",
    "    return unique_dataframe\n",
    "\n",
    "unidf = return_unique_values(twitter_df)\n",
    "print(unidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24315ab",
   "metadata": {},
   "source": [
    "### Data Exploration (EDA) & Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8223923",
   "metadata": {},
   "source": [
    "#### DateTime Parsing - Convert Date Format\n",
    "The `date` column contains date and time information as a object, let's to convert it into a DateTime for easier manipulation and to facilitate time series operations.\n",
    "\n",
    "Given the `date` format is (2009-04-06 22:19:49), to guarantee reliable and precise parsing of the date column in the dataset, error handling was incorporated into the datetime format code. Pandas can directly interpret each part of the datetime strings by defining the format (%Y-%m-%d %H:%M:%S), which prevents misunderstandings and improves parsing efficiency. The function can handle values that don't fit the required format by converting them to NaT when errors='coerce' is used. This preserves processing continuity and makes it simple to identify problems with the quality of the data. This technique is particularly helpful when getting your data ready for accurate and trustworthy time-series analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9ce7d201",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>date</th>\n",
       "      <th>user_handle</th>\n",
       "      <th>tweet_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1467810672</td>\n",
       "      <td>2009-04-06 22:19:49</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1467810917</td>\n",
       "      <td>2009-04-06 22:19:53</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1467811184</td>\n",
       "      <td>2009-04-06 22:19:57</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1467811193</td>\n",
       "      <td>2009-04-06 22:19:57</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1467811372</td>\n",
       "      <td>2009-04-06 22:20:00</td>\n",
       "      <td>joy_wolf</td>\n",
       "      <td>@Kwesidei not the whole crew</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      user_id                date    user_handle  \\\n",
       "0  1467810672 2009-04-06 22:19:49  scotthamilton   \n",
       "1  1467810917 2009-04-06 22:19:53       mattycus   \n",
       "2  1467811184 2009-04-06 22:19:57        ElleCTF   \n",
       "3  1467811193 2009-04-06 22:19:57         Karoli   \n",
       "4  1467811372 2009-04-06 22:20:00       joy_wolf   \n",
       "\n",
       "                                          tweet_text  \n",
       "0  is upset that he can't update his Facebook by ...  \n",
       "1  @Kenichan I dived many times for the ball. Man...  \n",
       "2    my whole body feels itchy and like its on fire   \n",
       "3  @nationwideclass no, it's not behaving at all....  \n",
       "4                      @Kwesidei not the whole crew   "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert the 'date' column to datetime specifying the exact format\n",
    "twitter_df['date'] = pd.to_datetime(twitter_df['date'], format='%Y-%m-%d %H:%M:%S', errors='coerce')\n",
    "\n",
    "# Check the first few entries to confirm the change\n",
    "twitter_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6fbae456",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datetime64[ns]\n"
     ]
    }
   ],
   "source": [
    "# Check the data types to confirm the change\n",
    "print(twitter_df['date'].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2eab0c40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter_df.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0cd07107",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1.599999e+06</td>\n",
       "      <td>1599999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.998818e+09</td>\n",
       "      <td>2009-05-31 07:26:27.994492416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.467811e+09</td>\n",
       "      <td>2009-04-06 22:19:49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.956916e+09</td>\n",
       "      <td>2009-05-28 23:01:17.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2.002102e+09</td>\n",
       "      <td>2009-06-02 03:08:55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2.177059e+09</td>\n",
       "      <td>2009-06-15 05:21:43.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2.329206e+09</td>\n",
       "      <td>2009-06-25 10:28:31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.935757e+08</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            user_id                           date\n",
       "count  1.599999e+06                        1599999\n",
       "mean   1.998818e+09  2009-05-31 07:26:27.994492416\n",
       "min    1.467811e+09            2009-04-06 22:19:49\n",
       "25%    1.956916e+09     2009-05-28 23:01:17.500000\n",
       "50%    2.002102e+09            2009-06-02 03:08:55\n",
       "75%    2.177059e+09     2009-06-15 05:21:43.500000\n",
       "max    2.329206e+09            2009-06-25 10:28:31\n",
       "std    1.935757e+08                            NaN"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d46ae3",
   "metadata": {},
   "source": [
    "The summary statistics for the user_id and date columns provide many insights into the user's activity and timing characteristics during the data collecting period. The user_id has a mean of around 1.998 billion, indicating a wide range of values, potentially due to a big user base or a wide encoding range for user identification. The difference between the minimum (1.467811e+09) and maximum (2.329206e+09) values, with a standard deviation of around 193.6 million, indicates significant variability in user ID assignments, which could be due to different periods of user registration or system changes affecting ID allocation. The date column ranges from April 6, 2009 to June 25, 2009, with the median occurring on June 2, 2009. \n",
    "\n",
    "The dataset may have captured seasonal user behaviour or events because of its temporal distribution, which shows that it covers almost three months in late spring to early summer. The mid-June and end-of-May 25th and 75th percentiles, respectively, point to a concentration of recordings around these dates, which may correspond with particular events or promotions that encourage user participation during these times."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e0583b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ababcab9",
   "metadata": {},
   "source": [
    "#### Histogram of Text Length and Word Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1530eda3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df['text_length'].plot(kind='hist', title='Distribution of Tweet Text Length', bins=20, alpha=0.7)\n",
    "plt.xlabel('Text Length')\n",
    "plt.show()\n",
    "\n",
    "tweets_df['word_count'].plot(kind='hist', title='Distribution of Tweet Word Count', bins=20, alpha=0.7)\n",
    "plt.xlabel('Word Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce5cb14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4bb1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Convert to Pandas DataFrame\n",
    "tweet_data = jdbc_df.limit(1000).toPandas()\n",
    "\n",
    "# Example visualization: Histogram\n",
    "tweet_data['tweet_length'] = tweet_data['tweet_text'].apply(len)\n",
    "tweet_data['tweet_length'].hist(bins=20)\n",
    "plt.xlabel(\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c20102",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e9796d76",
   "metadata": {},
   "source": [
    "#### Distributions\n",
    "Explore the distribution of key variables using histograms or box plots to understand their central tendencies and spread.\n",
    "\n",
    "#### Histograms: Useful for visualizing the distribution of numerical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eecf7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_ids = tweets_df['user_id']\n",
    "\n",
    "# Calculate histogram data\n",
    "counts, bin_edges = np.histogram(user_ids, bins=10)\n",
    "\n",
    "# Create a histogram\n",
    "plt.figure(figsize=(10, 6))  # Make it large enough for clarity\n",
    "plt.bar(bin_edges[:-1], counts, width=np.diff(bin_edges), edgecolor='black', align='edge', color='skyblue')\n",
    "plt.title('Distribution of User IDs')\n",
    "plt.xlabel('User ID Bins')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True)  \n",
    "\n",
    "# Remove all unnecessary tick marks and frame pieces\n",
    "plt.tick_params(top=False, right=False, left=True, bottom=True, labelleft=True, labelbottom=True)\n",
    "for spine in plt.gca().spines.values():\n",
    "    if spine.spine_type not in ['bottom', 'left']:  \n",
    "        spine.set_visible(False)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4567fcf",
   "metadata": {},
   "source": [
    "#### Box Plots: Good for detecting outliers and understanding the distribution's quartiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f730f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a box plot\n",
    "plt.figure(figsize=(8, 6)) \n",
    "plt.boxplot(user_ids, vert=False, widths=0.7, patch_artist=True, flierprops={'marker':'o', 'color':'red', 'markersize':5})\n",
    "plt.title('User ID Distribution')\n",
    "plt.xlabel('User IDs')\n",
    "plt.yticks([])  \n",
    "\n",
    "# Enhance data-ink ratio\n",
    "plt.gca().spines['top'].set_visible(False)\n",
    "plt.gca().spines['right'].set_visible(False)\n",
    "plt.gca().spines['left'].set_visible(False)  # Remove left spine if y-axis ticks are not informative\n",
    "plt.grid(True, linestyle='--', which='major', color='gray', alpha=0.5)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccce74d7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aaa3d79b",
   "metadata": {},
   "source": [
    "#### Plot histograms and density plots of user_id to understand user engagement and presence. \n",
    "Because the histogram helps to visualize how active users are in terms of the number of tweets they post. And the density plot provides a clear view of the distribution's shape, highlighting the typical user activity levels without the binning process of a histogram.\n",
    "\n",
    "First, set up correctly for visualising user activity using user_id and analysing the frequency of user interactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4103b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the frequency of each user_id\n",
    "user_activity = tweets_df['user_id'].value_counts()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a histogram of user activity\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(user_activity.values, bins=30, color='skyblue', edgecolor='black')\n",
    "plt.title('Histogram of User Activity')\n",
    "plt.xlabel('Number of Tweets per User')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True, which='both', linestyle='--', linewidth=0.5, alpha=0.7)  # Minimal grid use\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3e09d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# Create a density plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.kdeplot(user_activity.values, shade=True, color='blue', alpha=0.7)\n",
    "plt.title('Density Plot of User Activity')\n",
    "plt.xlabel('Number of Tweets per User')\n",
    "plt.ylabel('Density')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "780943f4",
   "metadata": {},
   "source": [
    "#### Pair Plots\n",
    "Use pair plots to visualise the relationships and distributions of each pair of variables in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a241c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a pair plot with a clean and minimalist design\n",
    "sns.set(style=\"white\")  # Sets the style of the plot to a simple white background\n",
    "pairplot = sns.pairplot(tweets_df, diag_kind='kde', plot_kws={'alpha': 0.6, 's': 80, \n",
    "                                                              'edgecolor': 'k'}, corner=True)\n",
    "for i in range(len(pairplot.axes)):\n",
    "    for j in range(len(pairplot.axes)):\n",
    "        if i != j:\n",
    "            pairplot.axes[i][j].set_visible(False)\n",
    "            if i == j:\n",
    "                pairplot.axes[i][j].set_ylabel('Density')\n",
    "\n",
    "# Add titles and labels\n",
    "plt.subplots_adjust(top=0.9)\n",
    "pairplot.fig.suptitle('Pairwise Plots of User Metrics', fontsize=16)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54109af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56e9e85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2ff77333",
   "metadata": {},
   "source": [
    "#### Correlation Matrix\n",
    "Investigate the correlations between numerical variables to better understand the links between the fields in the dataset.\n",
    "Creating a correlation matrix is an excellent way to visually and quantitatively investigate the correlations between numerical variables in your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581bd546",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Compute the correlation matrix\n",
    "corr_matrix = tweets_df.corr()\n",
    "\n",
    "# Print the correlation matrix\n",
    "print(corr_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4202fd4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a heatmap from the correlation matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(corr_matrix, annot=True, fmt=\".2f\", cmap='coolwarm', cbar=True, square=True, linewidths=.5)\n",
    "\n",
    "# Enhancing the heatmap\n",
    "plt.title('Correlation Matrix of Variables')\n",
    "plt.yticks(rotation=0)  # Rotate y-labels for better readability\n",
    "plt.xticks(rotation=90)  # Rotate x-labels for better readability\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87a6753",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2954b0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bb61fa8c",
   "metadata": {},
   "source": [
    "### Updated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "067cda35",
   "metadata": {},
   "source": [
    "### Text Data Analysis\n",
    "Analyzing the tweet texts, let's counting the number of words in each tweet.\n",
    "\n",
    "Before calling, x (the tweet text) is explicitly converted to a string using `str(x).split()` is a helpful conversion because it guards against errors that could occur if non-string data were unintentionally included in the tweet_text. It guarantees that the.split() method, which splits the text into words based on spaces, always has a string to work on by first converting everything to a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "5caa8284",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>date</th>\n",
       "      <th>user_handle</th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>text_length</th>\n",
       "      <th>word_count</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>clean_tweet_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1467810672</td>\n",
       "      <td>2009-04-06 22:19:49</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "      <td>111</td>\n",
       "      <td>21</td>\n",
       "      <td>0.000</td>\n",
       "      <td>is upset that he cant update his facebook by t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1467810917</td>\n",
       "      <td>2009-04-06 22:19:53</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "      <td>89</td>\n",
       "      <td>18</td>\n",
       "      <td>0.500</td>\n",
       "      <td>i dived many times for the ball managed to sav...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1467811184</td>\n",
       "      <td>2009-04-06 22:19:57</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>47</td>\n",
       "      <td>10</td>\n",
       "      <td>0.200</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1467811193</td>\n",
       "      <td>2009-04-06 22:19:57</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "      <td>111</td>\n",
       "      <td>21</td>\n",
       "      <td>-0.625</td>\n",
       "      <td>no its not behaving at all im mad why am i her...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1467811372</td>\n",
       "      <td>2009-04-06 22:20:00</td>\n",
       "      <td>joy_wolf</td>\n",
       "      <td>@Kwesidei not the whole crew</td>\n",
       "      <td>29</td>\n",
       "      <td>5</td>\n",
       "      <td>0.200</td>\n",
       "      <td>not the whole crew</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      user_id                date    user_handle  \\\n",
       "0  1467810672 2009-04-06 22:19:49  scotthamilton   \n",
       "1  1467810917 2009-04-06 22:19:53       mattycus   \n",
       "2  1467811184 2009-04-06 22:19:57        ElleCTF   \n",
       "3  1467811193 2009-04-06 22:19:57         Karoli   \n",
       "4  1467811372 2009-04-06 22:20:00       joy_wolf   \n",
       "\n",
       "                                          tweet_text  text_length  word_count  \\\n",
       "0  is upset that he can't update his Facebook by ...          111          21   \n",
       "1  @Kenichan I dived many times for the ball. Man...           89          18   \n",
       "2    my whole body feels itchy and like its on fire            47          10   \n",
       "3  @nationwideclass no, it's not behaving at all....          111          21   \n",
       "4                      @Kwesidei not the whole crew            29           5   \n",
       "\n",
       "   sentiment                                   clean_tweet_text  \n",
       "0      0.000  is upset that he cant update his facebook by t...  \n",
       "1      0.500  i dived many times for the ball managed to sav...  \n",
       "2      0.200     my whole body feels itchy and like its on fire  \n",
       "3     -0.625  no its not behaving at all im mad why am i her...  \n",
       "4      0.200                                 not the whole crew  "
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the length of each tweet and the word count\n",
    "twitter_df['word_count'] = twitter_df['tweet_text'].apply(lambda x: len(str(x).split()))\n",
    "twitter_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5bf710",
   "metadata": {},
   "source": [
    "The character count of each tweet and its total word count. An example of a moderately long tweet is the one by user \"scotthamilton\" in the first row, which has 111 characters and 21 words. The tweet from \"joy_wolf\" in the last row, on the other hand, is substantially shorter—just 29 characters and 5 words. These metrics are useful for examining tweet verbosity and content density. Applications such as sentiment analysis, user engagement research, and even social media language usage modelling can benefit from this kind of data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e43da3f",
   "metadata": {},
   "source": [
    "### Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "90724845",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!pip install textblob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5edd1c36",
   "metadata": {},
   "source": [
    "#### Text Cleaning\n",
    "For sentiment analysis, text needs to be cleaned for better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "fc931ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'@\\w+', '', text)  # Remove mentions\n",
    "    text = re.sub(r'http\\S+', '', text)  # Remove URLs\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)  # Remove punctuation and numbers\n",
    "    text = text.lower().strip()  # Convert to lower case and strip whitespaces\n",
    "    return text\n",
    "\n",
    "twitter_df['clean_tweet_text'] = twitter_df['tweet_text'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69030c0f",
   "metadata": {},
   "source": [
    "#### TextBlob to add sentiment scores to the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "8f19b153",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>clean_tweet_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>is upset that he cant update his facebook by t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "      <td>0.500</td>\n",
       "      <td>i dived many times for the ball managed to sav...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>0.200</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "      <td>-0.625</td>\n",
       "      <td>no its not behaving at all im mad why am i her...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@Kwesidei not the whole crew</td>\n",
       "      <td>0.200</td>\n",
       "      <td>not the whole crew</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          tweet_text  sentiment  \\\n",
       "0  is upset that he can't update his Facebook by ...      0.000   \n",
       "1  @Kenichan I dived many times for the ball. Man...      0.500   \n",
       "2    my whole body feels itchy and like its on fire       0.200   \n",
       "3  @nationwideclass no, it's not behaving at all....     -0.625   \n",
       "4                      @Kwesidei not the whole crew       0.200   \n",
       "\n",
       "                                    clean_tweet_text  \n",
       "0  is upset that he cant update his facebook by t...  \n",
       "1  i dived many times for the ball managed to sav...  \n",
       "2     my whole body feels itchy and like its on fire  \n",
       "3  no its not behaving at all im mad why am i her...  \n",
       "4                                 not the whole crew  "
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use libraries TextBlob \n",
    "from textblob import TextBlob\n",
    "\n",
    "# Function to get the polarity score\n",
    "def get_sentiment(text):\n",
    "    return TextBlob(text).sentiment.polarity\n",
    "\n",
    "twitter_df['sentiment'] = twitter_df['clean_tweet_text'].apply(get_sentiment)\n",
    "\n",
    "# Display the columns 'tweet_text', 'sentiment', and 'clean_tweet_text' \n",
    "twitter_df[['tweet_text', 'sentiment', 'clean_tweet_text']].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49335da5",
   "metadata": {},
   "source": [
    "The code effectively cleans and processes text for sentiment analysis, allowing for more accurate interpretation of emotional content in tweets. The findings show that TextBlob can distinguish between various emotional tones in tweets, ranging from strong negativity to positivity, based on lexical content, making it a useful tool for analysing sentiment in social media texts. This preprocessing and analysis pipeline serves as a solid foundation for future sentiment-based applications such as trend analysis, customer feedback, and long-term monitoring of public opinion."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b0a2ce8",
   "metadata": {},
   "source": [
    "## Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82df4a5b",
   "metadata": {},
   "source": [
    "### VADER (Valence Aware Dictionary)\n",
    "Use VADER (Valence Aware Dictionary) for sentiment analysis. The VADER was chosen because it is well-suited for social media text analysis due to its sensitivity to both polarity and emotional intensity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f7e6bf",
   "metadata": {},
   "source": [
    "#### Install Necessary Package\n",
    "First, install nltk library, which includes the VADER sentiment analysis tool. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "a8f2ae99",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install nltk\n",
    "#!pip install nltk scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f14bd1",
   "metadata": {},
   "source": [
    "####  Import Libraries and Load VADER\n",
    "Import necessary libraries and download the VADER lexicon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "6a43fc9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\rosil\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "# Initialize VADER\n",
    "sia = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431abbaf",
   "metadata": {},
   "source": [
    "#### Define a Function to Get Sentiment Scores and Apply the Sentiment Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "c35037ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          tweet_text  vader_sentiment\n",
      "0  is upset that he can't update his Facebook by ...          -0.7500\n",
      "1  @Kenichan I dived many times for the ball. Man...           0.4939\n",
      "2    my whole body feels itchy and like its on fire           -0.2500\n",
      "3  @nationwideclass no, it's not behaving at all....          -0.6597\n",
      "4                      @Kwesidei not the whole crew            0.0000\n"
     ]
    }
   ],
   "source": [
    "# Create a function that uses VADER to compute the sentiment scores.\n",
    "def get_vader_sentiment(text):\n",
    "    \n",
    "    # VADER outputs a dictionary of scores; 'compound' gives the normalized sentiment score\n",
    "    return sia.polarity_scores(text)['compound']\n",
    "\n",
    "# Use the function to calculate sentiment scores for each tweet.\n",
    "twitter_df['vader_sentiment'] = twitter_df['tweet_text'].apply(get_vader_sentiment)\n",
    "\n",
    "# Display the first few rows to check the sentiment scores results\n",
    "print(twitter_df[['tweet_text', 'vader_sentiment']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6e643e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5121673a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4aa6c69a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b58fa23",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_sentiment = tweets_df['sentiment'].resample('D').mean()\n",
    "daily_sentiment.plot(title='Daily Sentiment Score')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Sentiment Score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1206e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c82389aa",
   "metadata": {},
   "source": [
    "### Time Series Analysis and Forecasting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0271ccfb",
   "metadata": {},
   "source": [
    "#### Set Date as Index: For time series analysis, it's useful to have the date as the DataFrame index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8070cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df.set_index('date', inplace=True)\n",
    "tweets_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5330e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Aggregate Sentiment Over Time: \n",
    "Resample the sentiment data to daily, and calculate mean sentiment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3752904c",
   "metadata": {},
   "source": [
    "#### Trend Analysis: \n",
    "Ploting time series of counts of tweets per day to see if there's any visible trend or seasonality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d3811f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df.resample('D').size().plot(title='Daily Tweets')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Number of tweets')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3537c6",
   "metadata": {},
   "source": [
    "### Seasonality Check: \n",
    "Use seasonal decomposition to observe inherent seasonality in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27020dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install statsmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0dbd6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "result = seasonal_decompose(tweets_df.resample('D').size(), model='additive')\n",
    "result.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7efad0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0123a37a",
   "metadata": {},
   "source": [
    "### Correlation and Causation Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36891b21",
   "metadata": {},
   "source": [
    "#### Stationarity Check: \n",
    "Ensure the time series data is stationary, as this is a requirement for models like ARIMA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d44859",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import adfuller\n",
    "test_result = adfuller(daily_sentiment.dropna())\n",
    "print('ADF Statistic: %f' % test_result[0])\n",
    "print('p-value: %f' % test_result[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28afa86",
   "metadata": {},
   "source": [
    "#### Time Series Plot of Tweet Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682d59db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plotting the number of tweets over time\n",
    "twitter_df.set_index('date').resample('S').size().plot()\n",
    "plt.title('Tweet Frequency Over Time')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Number of Tweets')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b23477aa",
   "metadata": {},
   "source": [
    "### Time Series Preparation\n",
    "Ensure the DataFrame is sorted by date and set the date as an index for time series analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34785da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_df = twitter_df.sort_values(by='date')\n",
    "twitter_df.set_index('date', inplace=True)\n",
    "print(twitter_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f489ff79",
   "metadata": {},
   "source": [
    "### Forecasting:\n",
    "Implement time series forecasting models such as ARIMA or LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe08b70",
   "metadata": {},
   "source": [
    "### Data Visualization and Reporting\n",
    "Visualizing aspects of the data can further aid in understanding the distribution and relationships. \n",
    "\n",
    "Dynamic Dashboard Creation:\n",
    "Use an appropriate tool like Plotly and Dash to create an interactive dashboard. Prepare the data in notebook and export it for visualization.\n",
    "\n",
    "Documentation:\n",
    "Document the findings, methods, and justifications in the report according to the project guidelines."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d4d79cf",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "67668b00",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17bb926b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa280624",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ce7d8fa7",
   "metadata": {},
   "source": [
    "References:\n",
    "\n",
    "https://spark.apache.org/docs/latest/api/python/development/debugging.html#:~:text=1%202%204-,Py4j,its%20stack%20trace%2C%20as%20java.\n",
    "\n",
    "Apache Parquet Documentation\n",
    "The official documentation for the Parquet file format offers insights into its design, features, and benefits for using it in data storage and processing tasks.https://spark.apache.org/docs/latest/\n",
    "\n",
    "Databricks Resources\n",
    "Databricks, a company founded by the creators of Apache Spark, provides extensive resources, blogs, and tutorials on Spark and Parquet, including best practices for performance optimization.\n",
    "Link: Databricks - Apache Spark Resources\n",
    "\n",
    "This book by Holden Karau, Andy Konwinski, Patrick Wendell, and Matei Zaharia (O'Reilly Media) is a great resource to learn about Spark from the ground up, covering basic to advanced topics.\n",
    "ISBN: 978-1449358624"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a0c256",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
