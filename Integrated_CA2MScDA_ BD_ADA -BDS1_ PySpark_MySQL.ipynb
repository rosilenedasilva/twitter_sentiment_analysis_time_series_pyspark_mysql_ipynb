{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dfc4084a",
   "metadata": {},
   "source": [
    "## <font color='#0000FF'>MSC_DA_CA2 - INTEGRATEDCA2<font color='#1ABC9D'>\n",
    "### <font color='#'>**Advanced Data Analytics  & Big Data Storage and Processing**\n",
    "### <font color='#1ABC9C'>**Lecturer(s): David McQuaid and Muhammad Iqbal**\n",
    "------\n",
    "<font color='#1ABC9C'>**Student name / ID** // Rosilene Francisca da Silva - 2021090"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ad9063",
   "metadata": {},
   "source": [
    "### Data Integration and Preprocessing: Use Apache Spark to populate MySQL databases with large datasets efficiently. \n",
    "This process includes data integration and preprocessing, making it easier to manage data effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c80ae7b",
   "metadata": {},
   "source": [
    "This continuous assessment was required to identify and carry out an analysis of a large dataset gleaned from the Twitter API “ProjectTweets.csv”:\n",
    "\n",
    "The dataset is guaranteed to have an organised schema, dependable transactional integrity, strong query capabilities, and simple integration with Apache Spark by selecting MySQL over a NoSQL database. These advantages combine to make MySQL suitable for populating the information and further analytical processing.\n",
    "\n",
    "The dataset is guaranteed to have an organised schema, dependable transactional integrity, strong query capabilities, and simple integration with Apache Spark by selecting MySQL over a NoSQL database. These advantages combine to make MySQL the superior option for populating the information and doing further analytical processing.\n",
    "\n",
    "\"The dataset used in this analysis, ProjectTweets.csv, was provided by Professor [McQuaid] via the Moodle [https://moodle.cct.ie/mod/assign/view.php?id=44089]course [MSc in Data Analytics] CCT College Dublin on 18 April 2024.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c05ee89",
   "metadata": {},
   "source": [
    "### Configure Apache Spark to connect MySQL \n",
    "Start Spark Session:\n",
    "Set up PySpark, including necessary packages for MySQL connections."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7adebd",
   "metadata": {},
   "source": [
    "Rationale: This PySpark setup is designed to process data during a database comparison in an effective and scalable manner. With the application name \"Database Comparative Analysis,\" the SparkSession is created, and backward compatibility with timestamp formats is guaranteed by the legacy time parser policy. In order to efficiently handle huge data volumes, the memory configurations for the executors and driver are set to 8 GB each. Moreover, having 4 cores per executor optimises parallel processing.\n",
    "\n",
    "By dynamically allocating resources according to workload requirements, the cluster can grow from a minimum of 2 executors to a maximum of 100. Large dataset data shuffling speed is enhanced by increasing the shuffle divisions to 1000, which reduces execution time.\n",
    "\n",
    "Furthermore, smooth connectivity between Spark and MySQL for direct data intake and analysis is ensured by including the MySQL JDBC connector jar. This setup offers a scalable and highly effective environment appropriate for large-scale database comparison analysis projects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c121d9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c45c7de2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/11 18:40:04 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf, SparkContext\n",
    "import os\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Database Comparative Analysis\") \\\n",
    "    .config(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\") \\\n",
    "    .config(\"spark.driver.memory\", \"8g\") \\\n",
    "    .config(\"spark.executor.memory\", \"8g\") \\\n",
    "    .config(\"spark.executor.cores\", \"4\") \\\n",
    "    .config(\"spark.dynamicAllocation.enabled\", \"true\") \\\n",
    "    .config(\"spark.dynamicAllocation.minExecutors\", \"2\") \\\n",
    "    .config(\"spark.dynamicAllocation.maxExecutors\", \"100\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"1000\") \\\n",
    "    .config(\"spark.jars\", \"/home/hduser/Downloads/mysql-connector-j-8.0.33.jar\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1cb9ec26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.0.2.15:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.4.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Database Comparative Analysis</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7d7c0615d960>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "17c0369c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10.9.7\n"
     ]
    }
   ],
   "source": [
    "import py4j\n",
    "print(py4j.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24367509",
   "metadata": {},
   "source": [
    "Rationale:\n",
    "This PySpark setup is designed to process data during a database comparison in an effective and scalable manner. With the application name \"Database Comparative Analysis,\" the SparkSession is created, and backward compatibility with timestamp formats is guaranteed by the legacy time parser policy. In order to efficiently handle huge data volumes, the memory configurations for the executors and driver are set to 8 GB each. Moreover, having 4 cores per executor optimises parallel processing.\n",
    "\n",
    "By dynamically allocating resources according to workload requirements, the cluster can grow from a minimum of 2 executors to a maximum of 100. Large dataset data shuffling speed is enhanced by increasing the shuffle divisions to 1000, which reduces execution time. \n",
    "\n",
    "Furthermore, smooth connectivity between Spark and MySQL for direct data intake and analysis is ensured by including the MySQL JDBC connector jar. This setup offers a scalable and highly effective environment appropriate for large-scale database comparison analysis projects."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d2c919",
   "metadata": {},
   "source": [
    "#### Before attempting to read the data, let's test reading data or just establish a connection to identify if has any issue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "38cbc2a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-------------------+------------+-------------+--------------------+\n",
      "| id|   user_id|               date|query_status|  user_handle|          tweet_text|\n",
      "+---+----------+-------------------+------------+-------------+--------------------+\n",
      "|  1|1467810672|2009-04-06 22:19:49|    NO_QUERY|scotthamilton|is upset that he ...|\n",
      "+---+----------+-------------------+------------+-------------+--------------------+\n",
      "only showing top 1 row\n",
      "\n",
      "Connection established successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Test loading a simple query to ensure connectivity\n",
    "try:\n",
    "    jdbc_df = spark.read.format(\"jdbc\").option(\"url\", \"jdbc:mysql://localhost/twitter_data\").option(\n",
    "        \"driver\", \"com.mysql.cj.jdbc.Driver\").option(\"dbtable\", \"tweet_details\").option(\n",
    "        \"user\", \"root\").option(\"password\", \"password\").load()\n",
    "    jdbc_df.show(1)\n",
    "    print(\"Connection established successfully.\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(\"An error occurred:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bfa16e9",
   "metadata": {},
   "source": [
    "### Load the data from MySQL into a new DataFrame\n",
    "The Database `twitter_data` was created on MySQL following the `tweet_details`table.  \n",
    "Also defined and renamed the column names as \"id\", \"user_id\", \"date\", \"query_status\", \"user_handle\", and \"tweet_text\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "2774ce37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 1:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-------------------+------------+-------------+--------------------+\n",
      "| id|   user_id|               date|query_status|  user_handle|          tweet_text|\n",
      "+---+----------+-------------------+------------+-------------+--------------------+\n",
      "|  1|1467810672|2009-04-06 22:19:49|    NO_QUERY|scotthamilton|is upset that he ...|\n",
      "|  2|1467810917|2009-04-06 22:19:53|    NO_QUERY|     mattycus|@Kenichan I dived...|\n",
      "+---+----------+-------------------+------------+-------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Load the data from MySQL into a new DataFrame\n",
    "mysql_df = spark.read.format(\"jdbc\").options(\n",
    "    url=\"jdbc:mysql://localhost/twitter_data\",\n",
    "    driver=\"com.mysql.cj.jdbc.Driver\",\n",
    "    dbtable=\"tweet_details\",\n",
    "    user=\"root\",\n",
    "    password=\"password\"\n",
    ").load()\n",
    "\n",
    "# Show the first few rows of the DataFrame to verify the data\n",
    "mysql_df.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ac2d93",
   "metadata": {},
   "source": [
    "This code loaded the data from the tweet_details table in MySQL into a new DataFrame called mysql_df. As the data was displayed correctly, it indicates that the data was successfully read to MySQL from PySpark."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c390b498",
   "metadata": {},
   "source": [
    "#### Verify the DataFrame\n",
    "After load the dataset, it’s a good idea to check the type and first few rows of the DataFrame to ensure that everything is loaded correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "91605414",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- user_id: long (nullable = true)\n",
      " |-- date: timestamp (nullable = true)\n",
      " |-- query_status: string (nullable = true)\n",
      " |-- user_handle: string (nullable = true)\n",
      " |-- tweet_text: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 2:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-------------------+------------+-------------+--------------------+\n",
      "| id|   user_id|               date|query_status|  user_handle|          tweet_text|\n",
      "+---+----------+-------------------+------------+-------------+--------------------+\n",
      "|  1|1467810672|2009-04-06 22:19:49|    NO_QUERY|scotthamilton|is upset that he ...|\n",
      "|  2|1467810917|2009-04-06 22:19:53|    NO_QUERY|     mattycus|@Kenichan I dived...|\n",
      "|  3|1467811184|2009-04-06 22:19:57|    NO_QUERY|      ElleCTF|my whole body fee...|\n",
      "|  4|1467811193|2009-04-06 22:19:57|    NO_QUERY|       Karoli|@nationwideclass ...|\n",
      "|  5|1467811372|2009-04-06 22:20:00|    NO_QUERY|     joy_wolf|@Kwesidei not the...|\n",
      "+---+----------+-------------------+------------+-------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Print DataFrame schema\n",
    "mysql_df.printSchema()\n",
    "\n",
    "mysql_df.show(5, truncate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58181a86",
   "metadata": {},
   "source": [
    "The first part outlines the schema definition, detailing the structure of a dataset with fields such as id, user_id, date, query_status, user_handle, and tweet_text, specifying data types and their nullable status. The second part presents a sample of actual data extracted from this schema, displaying records of tweets with timestamps, user handles, and partial tweet contents. Each row represents individual tweets, marked with an ID and user ID, and classified under \"NO_QUERY\", indicating that these tweets were not part of a specific query filter. This output is typical of database management or data analysis tasks, providing insights into the nature of the data handled, likely for further processing or analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba690cd",
   "metadata": {},
   "source": [
    "#### Dataframe Information\n",
    "Show the number of rows and columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "f3050bad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 3:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 1599999, Number of columns: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "num_rows = mysql_df.count()\n",
    "num_columns = len(mysql_df.columns)\n",
    "print(f\"Number of rows: {num_rows}, Number of columns: {num_columns}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545bb96e",
   "metadata": {},
   "source": [
    "With 1,599,999 rows and 6 columns overall, the dataset has a sizable amount of data that may be thoroughly examined. Each row contains particular information possibly connected to tweets, with seven columns reflecting different dataset aspects. These features include numerical and categorical data. This dataset's extensive structure makes it possible to perform significant exploratory data analysis (EDA) and gain important insights about user behaviour and data patterns. It also makes it possible to perform more complex analysis or machine learning activities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618888f1",
   "metadata": {},
   "source": [
    "### Data Pre-Processing in PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "518a2ddd",
   "metadata": {},
   "source": [
    "#### Checking for missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "726d51d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 21:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No missing values found in any column.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Initialize a flag to track whether missing values are found\n",
    "from pyspark.sql.functions import col\n",
    "missing_values_found = False\n",
    "\n",
    "for column in mysql_df.columns:\n",
    "    missing_count = mysql_df.filter(col(column).isNull() | (col(column) == '')).count()\n",
    "    if missing_count > 0:\n",
    "        print(f\"Column {column} has {missing_count} missing values\")\n",
    "        missing_values_found = True\n",
    "\n",
    "# Check the flag after checking all columns, and print a message if no missing values were found\n",
    "if not missing_values_found:\n",
    "    print(\"No missing values found in any column.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c76c77",
   "metadata": {},
   "source": [
    "Based on output the dataset there is no missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "81b9be9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('id', 'int'),\n",
       " ('user_id', 'bigint'),\n",
       " ('date', 'timestamp'),\n",
       " ('query_status', 'string'),\n",
       " ('user_handle', 'string'),\n",
       " ('tweet_text', 'string')]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Displaying dtypes of columns\n",
    "mysql_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f01061",
   "metadata": {},
   "source": [
    "From the output the DataFrame schema, the columns `id` and `user_id` are numerical. \n",
    "The column `date`  has a type of timestamp, meaning it's a timestamp (or datetime) field, columns like `query_status` `user_handle`, and `tweet_text` has a type of string, meaning it's a textual field (or varchar in SQL)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bcf616f",
   "metadata": {},
   "source": [
    "#### Displaying some columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f7772b7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 24:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|user_id   |\n",
      "+----------+\n",
      "|1467810672|\n",
      "|1467810917|\n",
      "|1467811184|\n",
      "|1467811193|\n",
      "|1467811372|\n",
      "+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "mysql_df.select('user_id').show(n=5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "8d3e4121",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 25:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|date               |\n",
      "+-------------------+\n",
      "|2009-04-06 22:19:49|\n",
      "|2009-04-06 22:19:53|\n",
      "|2009-04-06 22:19:57|\n",
      "|2009-04-06 22:19:57|\n",
      "|2009-04-06 22:20:00|\n",
      "+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "mysql_df.select('date').show(n=5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f174f8ba",
   "metadata": {},
   "source": [
    "The output illustrates the temporal character of the data by showing the first five rows of the date column from the dataset. Each entry in the date column is a timestamp indicating when a particular event, such as a tweet, occurred."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575ad16a",
   "metadata": {},
   "source": [
    "#### Using the.agg() method and the aggregation functions min and max to find the earliest and latest dates will print the first and last tweet dates. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "74f2f276",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 26:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First tweet date: 2009-04-06 22:19:49\n",
      "Last tweet date: 2009-06-25 10:28:31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Import Aggregation Functions\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Finding the first and last tweet dates\n",
    "first_last_dates = mysql_df.agg(\n",
    "    F.min(\"date\").alias(\"first_date\"),\n",
    "    F.max(\"date\").alias(\"last_date\")\n",
    ").collect()[0]\n",
    "\n",
    "first_date = first_last_dates[\"first_date\"]\n",
    "last_date = first_last_dates[\"last_date\"]\n",
    "\n",
    "print(f\"First tweet date: {first_date}\")\n",
    "print(f\"Last tweet date: {last_date}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a0ec4c",
   "metadata": {},
   "source": [
    "The earliest and latest timestamps for tweets suggest that the dataset covers the period of April 6, 2009, to June 25, 2009. The last tweet was sent out on June 25, 2009, at 10:28:31, and the first one was sent out on April 6, 2009, at 22:19:49. \n",
    "\n",
    "According to this, the dataset includes tweets from over three months' worth of time, giving it a temporal range that is appropriate for studying tweet patterns and trends during that time. The outcome helps to clarify the historical bounds of the dataset and directs further data analysis techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "4271c440",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 29:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------------------------------------------------------------------------------------------------------------+\n",
      "|user_handle  |tweet_text                                                                                                     |\n",
      "+-------------+---------------------------------------------------------------------------------------------------------------+\n",
      "|scotthamilton|is upset that he can't update his Facebook by texting it... and might cry as a result  School today also. Blah!|\n",
      "|mattycus     |@Kenichan I dived many times for the ball. Managed to save 50%  The rest go out of bounds                      |\n",
      "|ElleCTF      |my whole body feels itchy and like its on fire                                                                 |\n",
      "|Karoli       |@nationwideclass no, it's not behaving at all. i'm mad. why am i here? because I can't see you all over there. |\n",
      "|joy_wolf     |@Kwesidei not the whole crew                                                                                   |\n",
      "+-------------+---------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Selecting multiple columns\n",
    "mysql_df.select(['user_handle','tweet_text']).show(n=5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87fceeec",
   "metadata": {},
   "source": [
    "The dataset's first rows provide insight into the `user_handle` and `tweet_text` columns. The user_handle column contains Twitter handles that indicate who wrote each tweet, but the tweet_text column has the actual text content of each tweet. For example, user scotthamilton complains about not being able to update Facebook via text, whereas mattycus describes diving for a ball. Other tweets show personal discomfort (ElleCTF), bewilderment (Karoli), and interactions with other users (joy_wolf). This selection captures various user moods and activities on Twitter within the dataset's duration, emphasising the diversity of content, which ranges from personal updates to social interactions. The truncate=False argument guarantees that the complete content of each tweet is displayed without truncation, providing a preview of the user's tweet interactions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa94228",
   "metadata": {},
   "source": [
    "#### Data Distribution\n",
    "Count distinct values of a categorical column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "7e4bda44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 30:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------+\n",
      "|query_status|  count|\n",
      "+------------+-------+\n",
      "|    NO_QUERY|1599999|\n",
      "+------------+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Selecting 'query_status' column\n",
    "mysql_df.groupBy(\"query_status\").count().show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa00cb9",
   "metadata": {},
   "source": [
    "The dataset's `query_status` column. In this instance, the output indicates that the query_status of NO_QUERY applies to all 1,599,999 tweets. This suggests that the query_status column has a consistent value across all rows, indicating that the field was not meaningfully used in the dataset or that none of the tweets were connected to any particular search query. The consistency shows that, absent other columns or contextual information, the column might not be pertinent for more investigation. All things considered, this discovery can direct data analysts to concentrate on other columns for significant insights while taking into account possible drop this column from further studies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "734dd5cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 33:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----+\n",
      "|    user_handle|count|\n",
      "+---------------+-----+\n",
      "|     megan_rice|   15|\n",
      "|         MeghTW|    1|\n",
      "|stranger_danger|   14|\n",
      "|       kyrabeth|    1|\n",
      "|    lovelylivxo|   16|\n",
      "|      tink68113|    1|\n",
      "|     Svalentyna|    1|\n",
      "|     bakerbelle|    1|\n",
      "|  somethingalex|    1|\n",
      "|     sexy_ass_T|    1|\n",
      "+---------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Selecting 'user_handle' column\n",
    "mysql_df.groupBy(\"user_handle\").count().show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a08493",
   "metadata": {},
   "source": [
    "The tweet count of distinct users is calculated by grouping the dataset by the `user_handle` column and counting the number of tweets linked with each user. The first ten rows of the output reveal a unique Twitter user and the number of tweets they have sent. For example, megan_rice has 15 tweets, stranger_danger has 14, and lovelylivxo has 16, yet MeghTW, kyrabeth, so on each have only one tweet. This distribution shows that, while some individuals are frequent tweeters, others have a low presence in the dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27021cd2",
   "metadata": {},
   "source": [
    "### Summary Statistics\n",
    "Generate summary statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "96573828",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 36:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+--------------------+------------+--------------------+--------------------+\n",
      "|summary|               id|             user_id|query_status|         user_handle|          tweet_text|\n",
      "+-------+-----------------+--------------------+------------+--------------------+--------------------+\n",
      "|  count|          1599999|             1599999|     1599999|             1599999|             1599999|\n",
      "|   mean|         800000.0|1.9988178841753244E9|        null| 4.325887521835714E9|                null|\n",
      "| stddev|461880.0710141109| 1.935756789172917E8|        null|5.162733218454889E10|                null|\n",
      "|    min|                1|          1467810672|    NO_QUERY|        000catnap000|                 ...|\n",
      "|    max|          1599999|          2329205794|    NO_QUERY|          zzzzeus111|ï¿½ï¿½ï¿½ï¿½ï¿½ß§...|\n",
      "+-------+-----------------+--------------------+------------+--------------------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Basic statistics\n",
    "mysql_df.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09029b04",
   "metadata": {},
   "source": [
    "The gives summary statistics for the dataset, shedding light on the distribution and features of each column. \n",
    "\n",
    "With a mean of 800,000.0 and a standard deviation of roughly 461,880, the id column shows a well-distributed set of unique identifiers ranging from 1 to 1,599,999. With a mean of almost 1.998 billion, the user_id column, which represents unique user identifiers, ranges from 1,467,810,672 to 2,329,205,794. All rows in the query_status column have the same value, NO_QUERY. The Twitter `user_handle` column span from 000catnap000 to zzzzeus111, indicating a wide range of users. \n",
    "Finally, a variety of text data are contained in the tweet_text column, including special characters that are evident in the maximum value ï¿½ï¿½ï¿½ï¿½ß§..., which may be indicative of encoding problems. A glimpse of the data distribution is given by the summary, which shows that although most columns are diverse and well-populated, the `query_status` column is uniform, and special characters would need to be cleaned up."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6d283b",
   "metadata": {},
   "source": [
    "#### Correlation Analysis\n",
    "Find correlations between numerical columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "bd77775d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numerical columns: ['id', 'user_id']\n"
     ]
    }
   ],
   "source": [
    "# List of numerical columns\n",
    "numerical_columns = [col for col, dtype in mysql_df.dtypes if dtype in ('int', 'bigint', 'double', 'float')]\n",
    "print(f\"Numerical columns: {numerical_columns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "e0bdc852",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 39:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+--------------------+\n",
      "|summary|               id|             user_id|\n",
      "+-------+-----------------+--------------------+\n",
      "|  count|          1599999|             1599999|\n",
      "|   mean|         800000.0|1.9988178841753244E9|\n",
      "| stddev|461880.0710141109| 1.935756789172917E8|\n",
      "|    min|                1|          1467810672|\n",
      "|    max|          1599999|          2329205794|\n",
      "+-------+-----------------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Summary statistics only for numerical columns\n",
    "mysql_df.select(numerical_columns).describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "904061ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation between id and user_id: 0.22304937463402058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 45:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation between user_id and id: 0.22304937463402053\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Calculate correlations between numerical columns\n",
    "from pyspark.sql.functions import corr\n",
    "\n",
    "for column1 in numerical_columns:\n",
    "    for column2 in numerical_columns:\n",
    "        if column1 != column2:\n",
    "            correlation = mysql_df.select(corr(column1, column2)).first()[0]\n",
    "            print(f\"Correlation between {column1} and {column2}: {correlation}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e9c51b",
   "metadata": {},
   "source": [
    "There is roughly a 0.223 correlation between `id` and `user_id`. Although there is a linear link between the two columns, it is not very strong, as indicated by the slight positive correlation between id and user_id. This discovery may help direct future research or data modelling by providing insight into the distribution of data and user behaviour."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff64bf48",
   "metadata": {},
   "source": [
    "#### Adding a New Column with the current timestamp & Extract only those tweets that contain the keyword \"summer\" in the tweet_text column.\n",
    "The `inserted_at` column will contain the timestamp indicating when each row was processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "cfd07660",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 48:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+--------------------+\n",
      "|               date|          tweet_text|         inserted_at|\n",
      "+-------------------+--------------------+--------------------+\n",
      "|2009-04-06 22:27:00|@jacobsummers Sor...|2024-05-11 18:41:...|\n",
      "|2009-04-06 23:40:34|@kimmyawesome Ohh...|2024-05-11 18:41:...|\n",
      "|2009-04-06 23:48:30|It's official! I'...|2024-05-11 18:41:...|\n",
      "|2009-04-07 00:12:36|ok my TWEET PEEP ...|2024-05-11 18:41:...|\n",
      "|2009-04-07 00:13:15|summer camp or su...|2024-05-11 18:41:...|\n",
      "|2009-04-07 00:34:43|Downy weather  Wh...|2024-05-11 18:41:...|\n",
      "|2009-04-07 00:35:12|Craaaaap. My Macb...|2024-05-11 18:41:...|\n",
      "|2009-04-07 00:41:46|@dadi_iyal and yo...|2024-05-11 18:41:...|\n",
      "|2009-04-07 01:57:26|@meatrack no more...|2024-05-11 18:41:...|\n",
      "|2009-04-07 01:57:35|searching for a j...|2024-05-11 18:41:...|\n",
      "+-------------------+--------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Filtering tweets containing a specific keyword 'summer'\n",
    "filtered_df = mysql_df.filter(mysql_df.tweet_text.contains(\"summer\"))\n",
    "\n",
    "# Add a current timestamp column for the insert time\n",
    "from pyspark.sql.functions import current_timestamp\n",
    "\n",
    "final_df = filtered_df.withColumn(\"inserted_at\", current_timestamp())\n",
    "\n",
    "# Select only the desired columns and print it\n",
    "final_df.select(\"date\", \"tweet_text\", \"inserted_at\").show(10, truncate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e161afb1",
   "metadata": {},
   "source": [
    "After filtering the dataset, this result displays the first ten rows of tweets that contain the keyword \"summer\". Each tweet is shown with its original timestamp (date), the tweet's text (tweet_text), and a new column (inserted_at) that shows the current timestamp when this information was processed. The date column providing information on the temporal distribution of tweets mentioning \"summer.\" For example, one user is undecided between \"summer camp or summer school,\" while another tweets about \"Downy Weather.\" The `inserted_at` column indicates that the data was processed, distinguishing between the original tweet timestamps and the time of processing. \n",
    "\n",
    "This allows for the tracking of when the filtered data was curated. Overall, this result displays a wide range of attitudes and interests relating to \"summer,\" from plans to technical concerns, providing useful insights into user interactions about this topic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0daff3e2",
   "metadata": {},
   "source": [
    "#### Word Frequency Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "a3474e87",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 49:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "|word|  count|\n",
      "+----+-------+\n",
      "|    |1184159|\n",
      "|  to| 552961|\n",
      "|   I| 496616|\n",
      "| the| 487501|\n",
      "|   a| 366211|\n",
      "|  my| 280025|\n",
      "| and| 275263|\n",
      "|   i| 249976|\n",
      "|  is| 217692|\n",
      "| you| 213871|\n",
      "+----+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Frequency of words in the \"tweet_text\" column\n",
    "from pyspark.sql.functions import explode, split\n",
    "mysql_df.withColumn(\"word\", explode(split(mysql_df[\"tweet_text\"], \"\\s+\"))).groupBy(\"word\").count().orderBy(\"count\", ascending=False).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc80df9",
   "metadata": {},
   "source": [
    "To analyze text data, paying particular attention to word frequency in the \"tweet_text\" column. The DataFrame is then grouped according to the \"word\" column, and the count() method is used to determine how many times each word appears. The most common terms are then highlighted by sorting the results in descending order using the \"count\" column. Lastly, the top ten outcomes are shown. The output displays the word counts. Common English terms like \"to,\" \"I,\" and \"the\" are followed by the most often occurring word, 1,184,159 times (blank spaces, probably from many consecutive spaces or formatting errors in tweets). \n",
    "\n",
    "Rationale: Understanding the dataset's typical word usage, identifying any data cleaning problems (such as the large number of blanks), and using the results as a foundation for additional text analysis tasks like sentiment analysis or theme modelling are all made possible by this analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "e35521db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['id', 'user_id', 'date', 'query_status', 'user_handle', 'tweet_text']\n"
     ]
    }
   ],
   "source": [
    "# Print current DataFrame columns\n",
    "print(mysql_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "973eb182",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('id', 'int'),\n",
       " ('user_id', 'bigint'),\n",
       " ('date', 'timestamp'),\n",
       " ('query_status', 'string'),\n",
       " ('user_handle', 'string'),\n",
       " ('tweet_text', 'string')]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Displaying dtypes of columns\n",
    "mysql_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8bfdc45",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "### Converting the dataset from MySQL to Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3163a1ce",
   "metadata": {},
   "source": [
    "#### Before converting the dataset to pandas, drop some unnecessary columns for further analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "a851d6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "mysql_df = mysql_df.drop('id','query_status')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "e34ec8a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 52:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+-------------+--------------------+\n",
      "|   user_id|               date|  user_handle|          tweet_text|\n",
      "+----------+-------------------+-------------+--------------------+\n",
      "|1467810672|2009-04-06 22:19:49|scotthamilton|is upset that he ...|\n",
      "|1467810917|2009-04-06 22:19:53|     mattycus|@Kenichan I dived...|\n",
      "+----------+-------------------+-------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "mysql_df.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb86ca02",
   "metadata": {},
   "source": [
    "#### Checking for missing Values again after dropped some columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "74f1e628",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 53:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+-----------+----------+\n",
      "|user_id|date|user_handle|tweet_text|\n",
      "+-------+----+-----------+----------+\n",
      "|      0|   0|          0|         0|\n",
      "+-------+----+-----------+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, sum\n",
    "\n",
    "null_count = mysql_df.select([sum(col(c).isNull().cast(\"int\")).alias(c) for c in mysql_df.columns])\n",
    "null_count.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de99f76",
   "metadata": {},
   "source": [
    "The output shows there are no missing values, then the dataset is ready for further processing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c191f1",
   "metadata": {},
   "source": [
    "### Collect the Dataset to Pandas \n",
    "Objective: Gather all rows from the Spark DataFrame (mysql_df) into a list of rows.\n",
    "\n",
    "Functionality:\n",
    ".collect(): Fetches all data rows from the distributed environment into the driver node as a list of Row objects.\n",
    "\n",
    "Rationale: Because this operation requires sufficient memory, the memory configurations for the executors and driver were set from 4 GB to 8 GB each, the configuration below on Spark was changed at the beginning, and unnecessary columns were removed before running the process:\n",
    "\n",
    "Setup before:\n",
    "\n",
    "    .config(\"spark.executor.cores\", \"4\") \\\n",
    "    .config(\"spark.dynamicAllocation.enabled\", \"true\") \\\n",
    "    .config(\"spark.dynamicAllocation.minExecutors\", \"2\") \\\n",
    "    .config(\"spark.dynamicAllocation.maxExecutors\", \"100\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"1000\") \\ \n",
    "\n",
    "Setup upgraded:\n",
    "\n",
    "     .config(\"spark.driver.memory\", \"8g\") \\\n",
    "    .config(\"spark.executor.memory\", \"8g\") \\\n",
    "    .config(\"spark.executor.cores\", \"4\") \\\n",
    "    .config(\"spark.dynamicAllocation.enabled\", \"true\") \\\n",
    "    .config(\"spark.dynamicAllocation.minExecutors\", \"2\") \\\n",
    "    .config(\"spark.dynamicAllocation.maxExecutors\", \"100\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"1000\") \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "9280b11b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Collect the Dataset to Pandas\n",
    "new_df = mysql_df.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f055c8e1",
   "metadata": {},
   "source": [
    "### Create Pandas DataFrame from MySQL\n",
    "Objective: Convert the list of rows into a Pandas DataFrame for further analysis.\n",
    "\n",
    "Functionality: pd.DataFrame(new_df): Initializes a Pandas DataFrame from the list of rows.\n",
    "twitter_df.columns = mysql_df.columns: Assigns the original column names from the Spark DataFrame (mysql_df) to the Pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "b5bcdb0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>date</th>\n",
       "      <th>user_handle</th>\n",
       "      <th>tweet_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1467810672</td>\n",
       "      <td>2009-04-06 22:19:49</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1467810917</td>\n",
       "      <td>2009-04-06 22:19:53</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1467811184</td>\n",
       "      <td>2009-04-06 22:19:57</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1467811193</td>\n",
       "      <td>2009-04-06 22:19:57</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1467811372</td>\n",
       "      <td>2009-04-06 22:20:00</td>\n",
       "      <td>joy_wolf</td>\n",
       "      <td>@Kwesidei not the whole crew</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      user_id                date    user_handle  \\\n",
       "0  1467810672 2009-04-06 22:19:49  scotthamilton   \n",
       "1  1467810917 2009-04-06 22:19:53       mattycus   \n",
       "2  1467811184 2009-04-06 22:19:57        ElleCTF   \n",
       "3  1467811193 2009-04-06 22:19:57         Karoli   \n",
       "4  1467811372 2009-04-06 22:20:00       joy_wolf   \n",
       "\n",
       "                                          tweet_text  \n",
       "0  is upset that he can't update his Facebook by ...  \n",
       "1  @Kenichan I dived many times for the ball. Man...  \n",
       "2    my whole body feels itchy and like its on fire   \n",
       "3  @nationwideclass no, it's not behaving at all....  \n",
       "4                      @Kwesidei not the whole crew   "
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create Pandas DataFrame from MySQL\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "twitter_df = pd.DataFrame(new_df)\n",
    "twitter_df.columns =  mysql_df.columns\n",
    "twitter_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "3d3b98e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1599999, 4)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778d0e50",
   "metadata": {},
   "source": [
    "The techniques was applyed to collect a large dataset from a Spark DataFrame (mysql_df) to a Pandas DataFrame (twitter_df) for further analysis. \n",
    "\n",
    "The command .collect() gathers all rows from the Spark DataFrame into a list of Row objects, which are then transformed to a Pandas DataFrame. The resulting twitter_df has 1,599,999 rows and 4 columns, indicating that the dataset is large enough to be explored and manipulated with Pandas. However, due to the scale of the dataset, attention should be given with memory utilisation, as Pandas operations might be more memory-intensive than Spark DataFrames."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989a2768",
   "metadata": {},
   "source": [
    "### Save the Dataset CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "97e73e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the Pandas DataFrame to a CSV file\n",
    "twitter_df.to_csv('twitter_df.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "ffb55a4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 401M\r\n",
      "-rw-r--r-- 1 hduser hadoopgroup  52K May 11 18:43 'Integrated_CA2MScDA_ BD_ADA -BDS1.ipynb'\r\n",
      "-rw-r--r-- 1 hduser hadoopgroup 101K May 10 17:02 'Integrated_CA2MScDA_ BD_ADA .ipynb'\r\n",
      "drwxr-xr-x 4 hduser hadoopgroup 4.0K May  6 10:02  MSCCA12023V2\r\n",
      "-rw-r--r-- 1 hduser hadoopgroup 2.4M May  5 15:20  mysql-connector-j-8.0.33.jar\r\n",
      "-rw-r--r-- 1 hduser hadoopgroup 219M Apr 28 09:50  ProjectTweets.csv\r\n",
      "-rw-r--r-- 1 hduser hadoopgroup   32 Apr 25 09:23  README.md\r\n",
      "-rw-r--r-- 1 hduser hadoopgroup 180M May 11 18:42  twitter_df.csv\r\n"
     ]
    }
   ],
   "source": [
    "!ls -lh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94e5a37",
   "metadata": {},
   "source": [
    "The collected DataFrame (twitter_df) is saved to a CSV file named twitter_df.csv via the to_csv() method. The index=False argument ensures that the index column does not appear in the output. This method facilitates data export, allowing the dataset to be shared, analysed, or preserved in CSV format. The generated CSV file has 1,599,999 rows and four columns (user_id, date, user_handle, and tweet_text), offering a complete and portable snapshot of Twitter data for further analysis.\n",
    "\n",
    "The command `!ls—lh` was applied to list all files in the current directory with detailed information like size, permissions, owner, and timestamps in a readable format and to confirm whether the twitter_df.csv dataset was stored back in the Hadoop directory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf52290",
   "metadata": {},
   "source": [
    "### Transitioning to Pandas for Continuous Analysis in a New Jupyter Notebook (Integrated_CA2MScDA_ BD_ADA_V2)\n",
    "\n",
    "#### Rationale:\n",
    "\n",
    "The decision to proceed with data analysis in Python using Pandas on the Windows platform was influenced by several factors. Primarily, challenges with integrating TensorFlow on the Linux virtual machine (VM) prompted a switch, compounded by the VM's restrictive memory limitations. Furthermore, pandas provide a more extensive and detailed library for data manipulation than PySpark, especially considering the breadth and depth of functionality available directly out of the box. Pandas' comprehensive functionality greatly expands its data processing capabilities on the Windows platform."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "653ca578",
   "metadata": {},
   "source": [
    "#### New Jupyter Notebook: Integrated_CA2MScDA_ BD_ADA_V2 / Using the same dataset that was renamed and processed  as a 'twitter_df.csv' dataset ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e983530d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
