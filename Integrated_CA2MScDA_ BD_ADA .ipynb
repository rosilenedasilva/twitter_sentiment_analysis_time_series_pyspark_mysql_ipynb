{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dfc4084a",
   "metadata": {},
   "source": [
    "## <font color='#0000FF'>MSC_DA_CA2 - INTEGRATEDCA2<font color='#1ABC9D'>\n",
    "### <font color='#'>**Advanced Data Analytics  & Big Data Storage and Processing**\n",
    "### <font color='#1ABC9C'>**Lecturer(s): David McQuaid and Muhammad Iqbal**\n",
    "------\n",
    "<font color='#1ABC9C'>**Student name / ID** // Rosilene Francisca da Silva - 2021090"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ad9063",
   "metadata": {},
   "source": [
    "### Configure Apache Spark\n",
    "Start Spark Session:\n",
    "Set up PySpark, including necessary packages for MongoDB and MySQL connections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c45c7de2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/05 16:00:45 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Database Comparative Analysis\") \\\n",
    "    .config(\"spark.mongodb.input.uri\", \"mongodb://127.0.0.1/twitter_data.tweets\") \\\n",
    "    .config(\"spark.mongodb.output.uri\", \"mongodb://127.0.0.1/twitter_data.tweets\") \\\n",
    "    .config(\"spark.jars\", \"/home/hduser/Downloads/mysql-connector-j-8.0.33.jar\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c05873b2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/05 16:00:45 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+----------------------------+--------+---------------+-------------------------------------------------------------------------------------------------------------------+\n",
      "|0  |1467810369|Mon Apr 06 22:19:45 PDT 2009|NO_QUERY|_TheSpecialOne_|@switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D|\n",
      "+---+----------+----------------------------+--------+---------------+-------------------------------------------------------------------------------------------------------------------+\n",
      "|1  |1467810672|Mon Apr 06 22:19:49 PDT 2009|NO_QUERY|scotthamilton  |is upset that he can't update his Facebook by texting it... and might cry as a result  School today also. Blah!    |\n",
      "|2  |1467810917|Mon Apr 06 22:19:53 PDT 2009|NO_QUERY|mattycus       |@Kenichan I dived many times for the ball. Managed to save 50%  The rest go out of bounds                          |\n",
      "+---+----------+----------------------------+--------+---------------+-------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Twitter Data Analysis\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Load the dataset from HDFS\n",
    "tweets_df = spark.read.csv(\"hdfs:///user/hduser/twitter_project/ProjectTweets.csv\", header=True, inferSchema=True)\n",
    "tweets_df.show(2, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76f388f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+----------------------------+------------+-------------+---------------------------------------------------------------------------------------------------------------+\n",
      "|id |user_id   |date                        |query_status|user_handle  |tweet_text                                                                                                     |\n",
      "+---+----------+----------------------------+------------+-------------+---------------------------------------------------------------------------------------------------------------+\n",
      "|1  |1467810672|Mon Apr 06 22:19:49 PDT 2009|NO_QUERY    |scotthamilton|is upset that he can't update his Facebook by texting it... and might cry as a result  School today also. Blah!|\n",
      "|2  |1467810917|Mon Apr 06 22:19:53 PDT 2009|NO_QUERY    |mattycus     |@Kenichan I dived many times for the ball. Managed to save 50%  The rest go out of bounds                      |\n",
      "|3  |1467811184|Mon Apr 06 22:19:57 PDT 2009|NO_QUERY    |ElleCTF      |my whole body feels itchy and like its on fire                                                                 |\n",
      "|4  |1467811193|Mon Apr 06 22:19:57 PDT 2009|NO_QUERY    |Karoli       |@nationwideclass no, it's not behaving at all. i'm mad. why am i here? because I can't see you all over there. |\n",
      "|5  |1467811372|Mon Apr 06 22:20:00 PDT 2009|NO_QUERY    |joy_wolf     |@Kwesidei not the whole crew                                                                                   |\n",
      "+---+----------+----------------------------+------------+-------------+---------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define column names\n",
    "column_names = [\"id\", \"user_id\", \"date\", \"query_status\", \"user_handle\", \"tweet_text\"]\n",
    "\n",
    "# Rename the columns\n",
    "tweets_df = tweets_df.toDF(*column_names)\n",
    "tweets_df.show(5, truncate=False) # Using truncate=False allows to see the full content."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284059bc",
   "metadata": {},
   "source": [
    "#### Adding a New Column with the Current Timestamp\n",
    "The `inserted_at` column will contain the timestamp indicating when each row was processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a51352a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+----------------------------+------------+-------------+---------------------------------------------------------------------------------------------------------------+--------------------------+\n",
      "|id |user_id   |date                        |query_status|user_handle  |tweet_text                                                                                                     |inserted_at               |\n",
      "+---+----------+----------------------------+------------+-------------+---------------------------------------------------------------------------------------------------------------+--------------------------+\n",
      "|1  |1467810672|Mon Apr 06 22:19:49 PDT 2009|NO_QUERY    |scotthamilton|is upset that he can't update his Facebook by texting it... and might cry as a result  School today also. Blah!|2024-05-05 16:01:01.533403|\n",
      "|2  |1467810917|Mon Apr 06 22:19:53 PDT 2009|NO_QUERY    |mattycus     |@Kenichan I dived many times for the ball. Managed to save 50%  The rest go out of bounds                      |2024-05-05 16:01:01.533403|\n",
      "+---+----------+----------------------------+------------+-------------+---------------------------------------------------------------------------------------------------------------+--------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# add or modify data fields\n",
    "from pyspark.sql.functions import current_timestamp\n",
    "\n",
    "# Adding a current timestamp column for the insert time\n",
    "tweets_df = tweets_df.withColumn(\"inserted_at\", current_timestamp())\n",
    "tweets_df.show(2, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0fe8d15",
   "metadata": {},
   "source": [
    "## Database Comparative Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a0d01a9",
   "metadata": {},
   "source": [
    "### Export Data from Spark to Databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5dc494df",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o60.save.\n: java.lang.ClassNotFoundException: com.mysql.cj.jdbc.Driver\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)\n\tat org.apache.spark.sql.execution.datasources.jdbc.DriverRegistry$.register(DriverRegistry.scala:46)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1(JDBCOptions.scala:103)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1$adapted(JDBCOptions.scala:103)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:103)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcOptionsInWrite.<init>(JDBCOptions.scala:246)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcOptionsInWrite.<init>(JDBCOptions.scala:250)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:47)\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:47)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:247)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_9441/3331734510.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0muser\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'root'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'password'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m ).mode('append').save()\n\u001b[0m",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m   1394\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1395\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1396\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1397\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1398\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o60.save.\n: java.lang.ClassNotFoundException: com.mysql.cj.jdbc.Driver\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)\n\tat org.apache.spark.sql.execution.datasources.jdbc.DriverRegistry$.register(DriverRegistry.scala:46)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1(JDBCOptions.scala:103)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1$adapted(JDBCOptions.scala:103)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:103)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcOptionsInWrite.<init>(JDBCOptions.scala:246)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcOptionsInWrite.<init>(JDBCOptions.scala:250)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:47)\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:47)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:247)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n"
     ]
    }
   ],
   "source": [
    "tweets_df.write.format('jdbc').options(\n",
    "    url='jdbc:mysql://localhost/twitter_data',\n",
    "    driver='com.mysql.cj.jdbc.Driver',\n",
    "    dbtable='tweets',\n",
    "    user='root',\n",
    "    password='password'\n",
    ").mode('append').save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc73fe95",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install mysql-connector-python\n",
    "import mysql.connector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88877138",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mysql.connector\n",
    "\n",
    "# Establish a MySQL connection\n",
    "config = {\n",
    "    \"user\": \"root\",  # MySQL username\n",
    "    \"password\": \"password\",  # MySQL password\n",
    "    \"host\": \"localhost\",  # Host where MySQL is running\n",
    "    \"database\": \"twitter_data\",  # Database name\n",
    "    \"raise_on_warnings\": True\n",
    "}\n",
    "\n",
    "# Create a connection object\n",
    "conn = mysql.connector.connect(**config)\n",
    "\n",
    "# Create a cursor object using the connection\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Query to select data\n",
    "query = \"SELECT * FROM tweets LIMIT 5;\"\n",
    "\n",
    "# Execute the query\n",
    "cursor.execute(query)\n",
    "\n",
    "# Fetch all rows from the last executed statement using fetchall()\n",
    "rows = cursor.fetchall()\n",
    "\n",
    "# Print rows\n",
    "for row in rows:\n",
    "    print(row)\n",
    "\n",
    "# Close the cursor and the connection\n",
    "cursor.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1280eb79",
   "metadata": {},
   "source": [
    "#### Export to MySQL:\n",
    "Use JDBC to write the DataFrame to MySQL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "627dbe3d",
   "metadata": {},
   "source": [
    "####  Include the JDBC Driver in Your Spark Session and Loading the Driver During Runtime:\n",
    "As PySpark session is already running and I cannot restart it with the --jars or --packages options, I decided dynamically add the JDBC driver to the classpath using the SparkContext:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d137ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkFiles\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Twitter Data Analysis\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "sc.addPyFile(\"/path/to/mysql-connector-java-8.0.xx.jar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5712db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame loaded with the data to write to MySQL\n",
    "tweets_df.write.format('jdbc').options(\n",
    "    url='jdbc:mysql://localhost/twitter_data',  # Replace 'twitter_data' with actual database name\n",
    "    driver='com.mysql.cj.jdbc.Driver',\n",
    "    dbtable='tweets',  # Replace 'tweets' with actual table name\n",
    "    user='root',  # Replace with actual MySQL username\n",
    "    password='password'  # Replace with your actual MySQL password\n",
    ").mode('append').save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc0ea9f",
   "metadata": {},
   "source": [
    "#### Export to MongoDB:\n",
    "Write the DataFrame to MongoDB:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c359646b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df.write.format(\"mongo\").option(\"uri\", \"mongodb://localhost/twitter_data.tweets\").mode(\"append\").save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c04541",
   "metadata": {},
   "source": [
    "### Configure YCSB for MySQL and Cassandra:\n",
    "Edit the YCSB properties files for MySQL and Cassandra to include the connection details, such as host, port, database/keyspace, and authentication info."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2764943",
   "metadata": {},
   "source": [
    "### Run YCSB Benchmarks\n",
    "#### Load Data and Perform Tests:\n",
    "#### For MySQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929548b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "./bin/ycsb load jdbc -P workloads/workloada -p db.driver=com.mysql.cj.jdbc.Driver -p \"db.url=jdbc:mysql://localhost/twitter_data?user=your_mysql_user&password=your_mysql_password\"\n",
    "./bin/ycsb run jdbc -P workloads/workloada -p db.driver=com.mysql.cj.jdbc.Driver -p \"db.url=jdbc:mysql://localhost/twitter_data?user=your_mysql_user&password=your_mysql_password\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f39e09d",
   "metadata": {},
   "source": [
    "#### For MongoDB:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb497a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "./bin/ycsb load mongodb -P workloads/workloada -p \"mongodb.url=mongodb://127.0.0.1:27017/twitter_data.tweets\"\n",
    "./bin/ycsb run mongodb -P workloads/workloada -p \"mongodb.url=mongodb://127.0.0.1:27017/twitter_data.tweets\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b04c856",
   "metadata": {},
   "source": [
    "### Analyze Results\n",
    "Compare Performance Metrics:\n",
    "Analyze the output from YCSB, focusing on metrics such as throughput, latency, and error rates. Use this data to compare the performance characteristics of MySQL and Cassandra under similar loads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92279ad6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6a13c06b",
   "metadata": {},
   "source": [
    "### Document Findings\n",
    "Report Generation:\n",
    "Document the setup, methodology, results, and analysis in a comprehensive report. Use charts and graphs to illustrate performance comparisons where applicable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b010c56",
   "metadata": {},
   "source": [
    "### Step 2: Data Processing in PySpark\n",
    "Before exporting the data to any database, you can apply necessary data transformations, filtering, or aggregations in PySpark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ab1c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of filtering tweets containing a specific keyword\n",
    "filtered_tweets_df = tweets_df.filter(tweets_df.tweet.contains(\"important\"))\n",
    "\n",
    "# might also want to add or modify data fields\n",
    "from pyspark.sql.functions import current_timestamp\n",
    "\n",
    "# Adding a current timestamp column for the insert time\n",
    "final_df = filtered_tweets_df.withColumn(\"inserted_at\", current_timestamp())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585b2aee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bdf4ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip uninstall matplotlib -y\n",
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f02ae83",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Importing the libraries.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "from matplotlib import cm\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2973bced",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Command to display all columns in the file.\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9546bb7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the Dataset \n",
    "tweets_df = pd.read_csv('ProjectTweets.csv')\n",
    "tweets_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d947cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set column's names\n",
    "tweets_df.columns = ['index', 'user_id', 'date', 'query_status', 'user_handle', 'tweet_text']\n",
    "tweets_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d7a185",
   "metadata": {},
   "source": [
    "After load the CSV file \"ProjectTweets.csv\" was noted that it has no headers. Renaming columns in a DataFrame makes the code cleaner and easier to comprehend and maintain. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f03763",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09841a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1819b85",
   "metadata": {},
   "source": [
    "Based on the first basic details, the dataset appears to be a collection of tweets, commonly can be use in sentiment analysis or other natural language processing tasks. \n",
    "\n",
    "Dataset Size and Integrity:\n",
    "The dataset contains approximately 1.6 million entries (1599999 entries, indexed from 0 to 1599998).\n",
    "Each column has the same number of non-null entries (1599999), indicating there are no missing values in any of the columns.\n",
    "Memory Usage: The dataframe is consuming approximately 73.2 MB of memory, which is relevant for understanding the computation resources needed to process this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7935cd07",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4577a43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('length of data is', len(tweets_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcbf05b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df.isnull().values.any()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d267c14",
   "metadata": {},
   "source": [
    "#### Filtering Data\n",
    "Filter rows based on some criteria. For example, to see tweets that have no query status:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c498121",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_query_tweets = tweets_df[tweets_df['query_status'] == 'NO_QUERY']\n",
    "print(no_query_tweets.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe13cbd4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tweets_df.query_status.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2d2528",
   "metadata": {},
   "source": [
    "The feature `query_status` shows whether any specific query key phrase has been used to collect the tweets; nonetheless, 100% of the entries in this column have the value \"NO_QUERY.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91173610",
   "metadata": {},
   "source": [
    "#### Display the Unique Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594f8194",
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in tweets_df.columns:\n",
    "    num_unique_values = tweets_df[column].nunique()\n",
    "    print(f\"Number of unique values in '{column}': {num_unique_values}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4786a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_unique_values(data_frame):\n",
    "    unique_dataframe = pd.DataFrame()\n",
    "    unique_dataframe['Features'] = data_frame.columns\n",
    "    uniques = []\n",
    "    for col in data_frame.columns:\n",
    "        u = data_frame[col].nunique()\n",
    "        uniques.append(u)\n",
    "    unique_dataframe['Uniques'] = uniques\n",
    "    return unique_dataframe\n",
    "\n",
    "unidf = return_unique_values(tweets_df)\n",
    "print(unidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe0a79b4",
   "metadata": {},
   "source": [
    "##### To begin with, dropping the columns that are unnecessary for the particular goal of sentiment analysis.The `index` and `query_status`, columns in this instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea583302",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping the 'index' and 'query_status' columns\n",
    "tweets_df.drop(columns=['index', 'query_status'], inplace=True)\n",
    "\n",
    "# Display the DataFrame to confirm changes\n",
    "tweets_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8223923",
   "metadata": {},
   "source": [
    "#### DateTime Parsing - Convert Date Format\n",
    "The `date` column contains date and time information as a string, let's to convert it into a DateTime for easier manipulation and to facilitate time series operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce7d201",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df['date'] = pd.to_datetime(tweets_df['date'], errors='coerce', format='%a %b %d %H:%M:%S PDT %Y')\n",
    "tweets_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fbae456",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tweets_df['date'].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eab0c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd07107",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d46ae3",
   "metadata": {},
   "source": [
    "The summary statistics for the user_id and date columns provide many insights into the user's activity and timing characteristics during the data collecting period. The user_id has a mean of around 1.998 billion, indicating a wide range of values, potentially due to a big user base or a wide encoding range for user identification. The difference between the minimum (1.467811e+09) and maximum (2.329206e+09) values, with a standard deviation of around 193.6 million, indicates significant variability in user ID assignments, which could be due to different periods of user registration or system changes affecting ID allocation. The date column ranges from April 6, 2009 to June 25, 2009, with the median occurring on June 2, 2009. \n",
    "\n",
    "The dataset may have captured seasonal user behaviour or events because of its temporal distribution, which shows that it covers almost three months in late spring to early summer. The mid-June and end-of-May 25th and 75th percentiles, respectively, point to a concentration of recordings around these dates, which may correspond with particular events or promotions that encourage user participation during these times."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0438625",
   "metadata": {},
   "source": [
    "#### Text Analysis\n",
    "Analyzing the tweet texts, let's counting the number of words in each tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327335a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the length of each tweet and the word count\n",
    "tweets_df['text_length'] = tweets_df['tweet_text'].apply(len)\n",
    "tweets_df['word_count'] = tweets_df['tweet_text'].apply(lambda x: len(x.split()))\n",
    "\n",
    "# Display these new metrics\n",
    "print(tweets_df[['tweet_text', 'text_length', 'word_count']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ea03cd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tweets_df['word_count'] = tweets_df['tweet_text'].apply(lambda x: len(str(x).split()))\n",
    "tweets_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2423af57",
   "metadata": {},
   "source": [
    "#### Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d2ad213",
   "metadata": {},
   "source": [
    "Using a simple library like TextBlob, to get a rough idea of the sentiment of each tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae1cf47",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1262292a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "# Calculate sentiment polarity\n",
    "tweets_df['sentiment'] = tweets_df['tweet_text'].apply(lambda text: TextBlob(text).sentiment.polarity)\n",
    "\n",
    "# Display tweets with their sentiment\n",
    "print(tweets_df[['tweet_text', 'sentiment']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c098cd0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "45ebb2ff",
   "metadata": {},
   "source": [
    "### Visualization\n",
    "Visualizing aspects of the data can further aid in understanding the distribution and relationships."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45864ab4",
   "metadata": {},
   "source": [
    "#### Time Series Plot of Tweet Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee390eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plotting the number of tweets over time\n",
    "tweets_df.set_index('date').resample('S').size().plot()\n",
    "plt.title('Tweet Frequency Over Time')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Number of Tweets')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ababcab9",
   "metadata": {},
   "source": [
    "#### Histogram of Text Length and Word Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1530eda3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df['text_length'].plot(kind='hist', title='Distribution of Tweet Text Length', bins=20, alpha=0.7)\n",
    "plt.xlabel('Text Length')\n",
    "plt.show()\n",
    "\n",
    "tweets_df['word_count'].plot(kind='hist', title='Distribution of Tweet Word Count', bins=20, alpha=0.7)\n",
    "plt.xlabel('Word Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c20102",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e9796d76",
   "metadata": {},
   "source": [
    "#### Distributions\n",
    "Explore the distribution of key variables using histograms or box plots to understand their central tendencies and spread.\n",
    "\n",
    "#### Histograms: Useful for visualizing the distribution of numerical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eecf7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_ids = tweets_df['user_id']\n",
    "\n",
    "# Calculate histogram data\n",
    "counts, bin_edges = np.histogram(user_ids, bins=10)\n",
    "\n",
    "# Create a histogram\n",
    "plt.figure(figsize=(10, 6))  # Make it large enough for clarity\n",
    "plt.bar(bin_edges[:-1], counts, width=np.diff(bin_edges), edgecolor='black', align='edge', color='skyblue')\n",
    "plt.title('Distribution of User IDs')\n",
    "plt.xlabel('User ID Bins')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True)  \n",
    "\n",
    "# Remove all unnecessary tick marks and frame pieces\n",
    "plt.tick_params(top=False, right=False, left=True, bottom=True, labelleft=True, labelbottom=True)\n",
    "for spine in plt.gca().spines.values():\n",
    "    if spine.spine_type not in ['bottom', 'left']:  \n",
    "        spine.set_visible(False)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4567fcf",
   "metadata": {},
   "source": [
    "#### Box Plots: Good for detecting outliers and understanding the distribution's quartiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f730f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a box plot\n",
    "plt.figure(figsize=(8, 6)) \n",
    "plt.boxplot(user_ids, vert=False, widths=0.7, patch_artist=True, flierprops={'marker':'o', 'color':'red', 'markersize':5})\n",
    "plt.title('User ID Distribution')\n",
    "plt.xlabel('User IDs')\n",
    "plt.yticks([])  \n",
    "\n",
    "# Enhance data-ink ratio\n",
    "plt.gca().spines['top'].set_visible(False)\n",
    "plt.gca().spines['right'].set_visible(False)\n",
    "plt.gca().spines['left'].set_visible(False)  # Remove left spine if y-axis ticks are not informative\n",
    "plt.grid(True, linestyle='--', which='major', color='gray', alpha=0.5)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccce74d7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aaa3d79b",
   "metadata": {},
   "source": [
    "#### Plot histograms and density plots of user_id to understand user engagement and presence. \n",
    "Because the histogram helps to visualize how active users are in terms of the number of tweets they post. And the density plot provides a clear view of the distribution's shape, highlighting the typical user activity levels without the binning process of a histogram.\n",
    "\n",
    "First, set up correctly for visualising user activity using user_id and analysing the frequency of user interactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4103b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the frequency of each user_id\n",
    "user_activity = tweets_df['user_id'].value_counts()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a histogram of user activity\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(user_activity.values, bins=30, color='skyblue', edgecolor='black')\n",
    "plt.title('Histogram of User Activity')\n",
    "plt.xlabel('Number of Tweets per User')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True, which='both', linestyle='--', linewidth=0.5, alpha=0.7)  # Minimal grid use\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3e09d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# Create a density plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.kdeplot(user_activity.values, shade=True, color='blue', alpha=0.7)\n",
    "plt.title('Density Plot of User Activity')\n",
    "plt.xlabel('Number of Tweets per User')\n",
    "plt.ylabel('Density')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "780943f4",
   "metadata": {},
   "source": [
    "#### Pair Plots\n",
    "Use pair plots to visualise the relationships and distributions of each pair of variables in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a241c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a pair plot with a clean and minimalist design\n",
    "sns.set(style=\"white\")  # Sets the style of the plot to a simple white background\n",
    "pairplot = sns.pairplot(tweets_df, diag_kind='kde', plot_kws={'alpha': 0.6, 's': 80, \n",
    "                                                              'edgecolor': 'k'}, corner=True)\n",
    "for i in range(len(pairplot.axes)):\n",
    "    for j in range(len(pairplot.axes)):\n",
    "        if i != j:\n",
    "            pairplot.axes[i][j].set_visible(False)\n",
    "            if i == j:\n",
    "                pairplot.axes[i][j].set_ylabel('Density')\n",
    "\n",
    "# Add titles and labels\n",
    "plt.subplots_adjust(top=0.9)\n",
    "pairplot.fig.suptitle('Pairwise Plots of User Metrics', fontsize=16)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54109af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56e9e85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2ff77333",
   "metadata": {},
   "source": [
    "#### Correlation Matrix\n",
    "Investigate the correlations between numerical variables to better understand the links between the fields in the dataset.\n",
    "Creating a correlation matrix is an excellent way to visually and quantitatively investigate the correlations between numerical variables in your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581bd546",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Compute the correlation matrix\n",
    "corr_matrix = tweets_df.corr()\n",
    "\n",
    "# Print the correlation matrix\n",
    "print(corr_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4202fd4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a heatmap from the correlation matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(corr_matrix, annot=True, fmt=\".2f\", cmap='coolwarm', cbar=True, square=True, linewidths=.5)\n",
    "\n",
    "# Enhancing the heatmap\n",
    "plt.title('Correlation Matrix of Variables')\n",
    "plt.yticks(rotation=0)  # Rotate y-labels for better readability\n",
    "plt.xticks(rotation=90)  # Rotate x-labels for better readability\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87a6753",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bb61fa8c",
   "metadata": {},
   "source": [
    "### Text Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "067cda35",
   "metadata": {},
   "source": [
    "#### Sentiment Analysis: \n",
    "Planing to forecast sentiment trends, let's first need to compute the sentiment scores of the tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90724845",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "tweets_df['sentiment'] = tweets_df['tweet_text'].apply(lambda x: TextBlob(x).sentiment.polarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa6c69a",
   "metadata": {},
   "source": [
    "#### Aggregate Sentiment Over Time: \n",
    "Resample the sentiment data to daily, and calculate mean sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b58fa23",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_sentiment = tweets_df['sentiment'].resample('D').mean()\n",
    "daily_sentiment.plot(title='Daily Sentiment Score')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Sentiment Score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1206e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c82389aa",
   "metadata": {},
   "source": [
    "### Time Series Analysis and Forecasting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0271ccfb",
   "metadata": {},
   "source": [
    "#### Set Date as Index: For time series analysis, it's useful to have the date as the DataFrame index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8070cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df.set_index('date', inplace=True)\n",
    "tweets_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5330e7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3752904c",
   "metadata": {},
   "source": [
    "#### Trend Analysis: \n",
    "Ploting time series of counts of tweets per day to see if there's any visible trend or seasonality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d3811f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df.resample('D').size().plot(title='Daily Tweets')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Number of tweets')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3537c6",
   "metadata": {},
   "source": [
    "#### Seasonality Check: \n",
    "Use seasonal decomposition to observe inherent seasonality in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27020dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install statsmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0dbd6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "result = seasonal_decompose(tweets_df.resample('D').size(), model='additive')\n",
    "result.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7efad0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0123a37a",
   "metadata": {},
   "source": [
    "### Correlation and Causation Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36891b21",
   "metadata": {},
   "source": [
    "#### Stationarity Check: \n",
    "Ensure the time series data is stationary, as this is a requirement for models like ARIMA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d44859",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import adfuller\n",
    "test_result = adfuller(daily_sentiment.dropna())\n",
    "print('ADF Statistic: %f' % test_result[0])\n",
    "print('p-value: %f' % test_result[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2f9662",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4fb81403",
   "metadata": {},
   "source": [
    "### Data Splitting: \n",
    "Decide on the training and testing periods for the models. Typically, the latter part of the data is held out for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fdccfc7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9fde5b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6b9706e1",
   "metadata": {},
   "source": [
    "### Apache Spark\n",
    "\n",
    "Setup the Notebook Environment.\n",
    "\n",
    "Import the required libraries and set up the Spark session to interface with Cassandra and handle data processing needs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9554a590",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf, SparkContext\n",
    "import os\n",
    "\n",
    "# Set the SPARK_LOCAL_IP environment variable if necessary\n",
    "# os.environ['SPARK_LOCAL_IP'] = '10.0.2.15' \n",
    "\n",
    "# Initialize a Spark session and Configure Spark to interact with Cassandra\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ProjectTweets.csv\") \\\n",
    "    .config(\"spark.master\", \"local\") \\\n",
    "    .config(\"spark.ui.port\", \"4050\") \\\n",
    "    .config(\"spark.cassandra.connection.host\", \"localhost\") \\\n",
    "    .config(\"spark.cassandra.connection.port\", \"9042\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d652472",
   "metadata": {},
   "source": [
    "#### Data Loading and Preliminary Analysis\n",
    "\n",
    "First, load the CSV file \"ProjectTweets.csv\" without assuming it has headers. With specify header=False to tell Spark that the first row should not be treated as headers, and then it will explicitly provide the column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9917a203",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df = spark.read.csv(\"file:///home/hduser/Desktop/INTEGRATEDCA2V2/ProjectTweets.csv\", \n",
    "                                   header=False, inferSchema=True)\n",
    "tweets_df.show(2, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c712c8",
   "metadata": {},
   "source": [
    "### Data Cleaning and Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883ed0d9",
   "metadata": {},
   "source": [
    "Because this dataset lacks column titles, column names must be assigned manually when the data is loaded into Spark. This guarantees that the data processing and analysis processes are simple to comprehend and manage. \n",
    "\n",
    "Let's implement it properly,using the `toDF()` method as it given the data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c170cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define column names\n",
    "column_names = [\"id\", \"user_id\", \"date\", \"query_status\", \"user_handle\", \"tweet_text\"]\n",
    "\n",
    "# Rename the columns\n",
    "tweets_df = tweets_df.toDF(*column_names)\n",
    "tweets_df.show(5, truncate=False) # Using truncate=False allows to see the full content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d46c82c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print current DataFrame columns to understand the naming\n",
    "print(tweets_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0821eeaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying dtypes of columns\n",
    "tweets_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359a003c",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(tweets_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58e2788",
   "metadata": {},
   "source": [
    "#### Verify the DataFrame\n",
    "After renaming the columns, its a good idea to check the first few rows of the DataFrame to ensure that everything is loaded correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053d45eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print DataFrame schema\n",
    "tweets_df.printSchema()\n",
    "\n",
    "tweets_df.show(5, truncate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e7633d9",
   "metadata": {},
   "source": [
    "Based on the first details, the dataset appears to be a collection of tweets, commonly can be use in sentiment analysis or other natural language processing tasks. \n",
    "\n",
    "The dataset columns like 'id' and 'user_id' employ Integer data types, which are great for unique IDs and indexing. Meanwhile, columns labelled 'date', 'query_status', 'user_handle', and 'tweet_text' are of the String type, which can include a wide range of textual and category information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b83d20c",
   "metadata": {},
   "source": [
    "#### Saving the DataFrame\n",
    "Consider saving this DataFrame in a more effective format, such as Parquet, in case it needs to be reused with the correct column names in subsequent sessions or analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124455c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the DataFrame for future use\n",
    "tweets_df.write.parquet(\"/path/to/save/tweets_df.parquet\", mode=\"overwrite\")\n",
    "\n",
    "# Load it whenever needed\n",
    "tweets_df = spark.read.parquet(\"/path/to/save/tweets_df.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0be799",
   "metadata": {},
   "source": [
    "Saving a DataFrame in a format such as Parquet is extremely useful, particularly when dealing with huge datasets or when the same data will be reused across numerous sessions or projects. Here are some of the primary reasons why saving the DataFrame in a format like Parquet is recommended: Storage and I/O operations are more efficient, with built-in schema preservation and the ability to handle complex data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d29b394",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of records\n",
    "print(f\"Class: {type(tweets_df)}\")\n",
    "print(f\"Total records: {tweets_df.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "758c5810",
   "metadata": {},
   "source": [
    "The dataset contains approximately 1.6 million entries. \n",
    "Scalability of the dataset: The dataset's 1.6 million items make it suitable for training increasingly sophisticated data-driven models, particularly deep learning models used in natural language processing and other machine learning applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740aa10b",
   "metadata": {},
   "source": [
    "#### Checking for Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c36813",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Get number of non-nulls per column by subtracting count of nulls from total entries\n",
    "total_entries = tweets_df.count()\n",
    "for column in tweets_df.columns:\n",
    "    non_nulls = total_entries - tweets_df.filter(tweets_df[column].isNull()).count()\n",
    "    print(f\"Column '{column}' has {non_nulls} non-null entries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5daaee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a flag to track whether missing values are found\n",
    "missing_values_found = False\n",
    "\n",
    "for column in tweets_df.columns:\n",
    "    missing_count = tweets_df.filter(col(column).isNull() | (col(column) == '')).count()\n",
    "    if missing_count > 0:\n",
    "        print(f\"Column {column} has {missing_count} missing values\")\n",
    "        missing_values_found = True\n",
    "\n",
    "# Check the flag after checking all columns, and print a message if no missing values were found\n",
    "if not missing_values_found:\n",
    "    print(\"No missing values found in any column.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35308a88",
   "metadata": {},
   "source": [
    "After checking each column in the dataset for null entries or empty strings, no such missing values were found across all columns, indicating that the dataset is complete and ready for further analysis without the requirement for data cleaning to fill or remove missing entries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3f8c8a",
   "metadata": {},
   "source": [
    "### Data Exploration (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d74f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "print(pyspark.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00a96e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import py4j\n",
    "print(py4j.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d677f605",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af6ab19",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Convert Spark DataFrame to Pandas DataFrame\n",
    "pandas_df = tweets_df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d44d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first few rows of the DataFrame\n",
    "print(pandas_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faca392b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics for numerical columns\n",
    "print(pandas_df.describe())\n",
    "\n",
    "# Check for missing values\n",
    "print(pandas_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7528126b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic statistics\n",
    "tweets_df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85a53c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying the values of a particular column\n",
    "tweets_df.select('date').show(n=10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d165f2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting multiple columns\n",
    "tweets_df.select(['user_id','user_handle']).show(n=10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1092977c",
   "metadata": {},
   "source": [
    "### HandySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dedfe4bc",
   "metadata": {},
   "source": [
    "HandySpark is very helpful when moving from Pandas to PySpark since it creates a bridge by giving Spark DataFrames an interface similar to Pandas. Improved statistical analysis tools that are not easily accessible in PySpark, improved visualisation possibilities straight from DataFrames, and simpler, more intuitive data manipulation are all made possible by this connection. With features like stratified sampling, HandySpark also makes managing unbalanced datasets easier and provides improved methods for handling missing data. HandySpark retains the scalable architecture of Spark, even though it adds some overhead when compared to native PySpark. This makes it a viable option for individuals who want to combine the simplicity of Pandas with the capability of distributed computing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b34273",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install handyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e4eec3",
   "metadata": {},
   "source": [
    "#### Distributions\n",
    "Examine the distribution of key variables using histograms or box plots to understand their central tendencies and spread."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4bca374",
   "metadata": {},
   "source": [
    "#### Histograms: Useful for visualizing the distribution of numerical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebff6d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f399c9",
   "metadata": {},
   "source": [
    "Caching can prevent recomputation of the DataFrame from the source data, improving performance for repeated queries and transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47df1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from handyspark import *\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create or get the existing Spark session\n",
    "spark = SparkSession.builder.appName(\"User ID Histogram with HandySpark\").getOrCreate()\n",
    "\n",
    "# Convert your PySpark DataFrame to a HandySpark DataFrame\n",
    "hdf = tweets_df.toHandy()\n",
    "\n",
    "# Using HandySpark to plot a histogram\n",
    "hdf.cols['user_id'].hist(bins=10, figsize=(8, 6), color='blue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46702680",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_data = tweets_df.select(\"user_id\").rdd.flatMap(lambda x: x).histogram(10)\n",
    "pd.DataFrame(list(zip(*numeric_data)), columns=[\"bin\", \"frequency\"]).plot(kind=\"bar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681a3f30",
   "metadata": {},
   "source": [
    "#### Box Plots: Good for detecting outliers and understanding the distribution's quartiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb12b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df.toPandas().boxplot(column=['user_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0edc8e05",
   "metadata": {},
   "source": [
    "#### Pair Plots\n",
    "Use pair plots to visualise the relationships and distributions of each pair of variables in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aab3c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_pd = tweets_df.select(numeric_features).sample(False, 0.1).toPandas()\n",
    "sns.pairplot(sampled_pd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d653e39",
   "metadata": {},
   "source": [
    "#### Correlation Matrix\n",
    "Investigate the correlations between numerical variables to better understand the links between the fields in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97bda79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features = [t[0] for t in tweets_df.dtypes if t[1] == 'int' or t[1] == 'double']\n",
    "sampled_data = tweets_df.select(numeric_features).sample(False, 0.1).toPandas()\n",
    "correlations = sampled_data.corr()\n",
    "sns.heatmap(correlations, annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda8e83a",
   "metadata": {},
   "source": [
    "### Categorical Data Analysis:\n",
    "\n",
    "If the data contains categorical information, use bar or pie charts to visualise the distribution of categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f826717",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df.groupBy(\"some_categorical_column\").count().toPandas().plot(kind='bar', x='some_categorical_column', y='count')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45dfebb4",
   "metadata": {},
   "source": [
    "#### Pie Charts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b4f5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "pie_data = tweets_df.groupBy(\"another_categorical_column\").count().toPandas()\n",
    "pie_data.plot(kind='pie', y='count', labels=pie_data['another_categorical_column'], autopct='%1.1f%%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca965ff0",
   "metadata": {},
   "source": [
    "#### Bar Charts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79fd5ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df.groupBy(\"some_categorical_column\").count().toPandas().plot(kind='bar', x='some_categorical_column', y='count')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b594f492",
   "metadata": {},
   "source": [
    "#### Time Series Analysis\n",
    "For data with a temporal component, plot time series to see trends, cycles, or irregular patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92343ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_series_data = tweets_df.groupBy(\"date_column\").agg({\"numerical_column\": \"mean\"})\n",
    "time_series_data.toPandas().plot(x='date_column', y='mean_numerical_column')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b2eed7",
   "metadata": {},
   "source": [
    "#### Text Data Analysis\n",
    "For datasets containing text data, such as tweets, conduct text-specific analyses:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c789b50",
   "metadata": {},
   "source": [
    "#### Word Frequency Count:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c829bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import explode, split\n",
    "tweets_df.withColumn(\"word\", explode(split(tweets_df[\"text\"], \"\\s+\"))).groupBy(\"word\").count().orderBy(\"count\", ascending=False).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28dd17e",
   "metadata": {},
   "source": [
    "#### Word Cloud:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20b4b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "text_data = tweets_df.select(\"text\").rdd.flatMap(lambda x: x).collect()\n",
    "text_string = \" \".join(text_data)\n",
    "wordcloud = WordCloud().generate(text_string)\n",
    "\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30cca4b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "145b6685",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4559b90",
   "metadata": {},
   "source": [
    "Prepare the data for analysis, which may include parsing dates or extracting specific features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411d965e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_date\n",
    "\n",
    "# Convert 'date' to a proper date format if needed\n",
    "clean_tweets_df = clean_tweets_df.withColumn(\"date\", to_date(\"date\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0302f912",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1fa56435",
   "metadata": {},
   "source": [
    "### Sentiment Analysis \n",
    "\n",
    "Choose Sentiment Analysis Technique:\n",
    "Depending on the libraries available, setup for sentiment analysis might involve using an NLP library or building a custom model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a3bc28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example using TextBlob for sentiment analysis (simplistic and illustrative)\n",
    "from textblob import TextBlob\n",
    "\n",
    "def sentiment_analysis(text):\n",
    "    return TextBlob(text).sentiment.polarity\n",
    "\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import FloatType\n",
    "\n",
    "sentiment_udf = udf(sentiment_analysis, FloatType())\n",
    "\n",
    "# Apply sentiment analysis\n",
    "sentiment_df = clean_tweets_df.withColumn(\"sentiment\", sentiment_udf(clean_tweets_df[\"text\"]))\n",
    "sentiment_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35906e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bd9baef7",
   "metadata": {},
   "source": [
    "### Time Series Analysis and Forecasting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0309d910",
   "metadata": {},
   "source": [
    "#### Aggregate Data for Time Series:\n",
    "Prepare the data for time series analysis, typically needing aggregation by date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4c6c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import avg\n",
    "\n",
    "daily_sentiment = sentiment_df.groupBy(\"date\").agg(avg(\"sentiment\").alias(\"avg_sentiment\"))\n",
    "daily_sentiment.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34785da3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f489ff79",
   "metadata": {},
   "source": [
    "#### Forecasting:\n",
    "Implement time series forecasting models such as ARIMA or LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe08b70",
   "metadata": {},
   "source": [
    "#### Visualization and Reporting\n",
    "Dynamic Dashboard Creation:\n",
    "Use an appropriate tool like Plotly or PowerBI to create an interactive dashboard. Prepare the data in notebook and export it for visualization.\n",
    "\n",
    "Documentation:\n",
    "Document the findings, methods, and justifications in the report according to the project guidelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b4cd7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b15b1387",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17bb926b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa280624",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ce7d8fa7",
   "metadata": {},
   "source": [
    "References:\n",
    "\n",
    "Apache Parquet Documentation\n",
    "The official documentation for the Parquet file format offers insights into its design, features, and benefits for using it in data storage and processing tasks.https://spark.apache.org/docs/latest/\n",
    "\n",
    "Databricks Resources\n",
    "Databricks, a company founded by the creators of Apache Spark, provides extensive resources, blogs, and tutorials on Spark and Parquet, including best practices for performance optimization.\n",
    "Link: Databricks - Apache Spark Resources\n",
    "\n",
    "This book by Holden Karau, Andy Konwinski, Patrick Wendell, and Matei Zaharia (O'Reilly Media) is a great resource to learn about Spark from the ground up, covering basic to advanced topics.\n",
    "ISBN: 978-1449358624"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a0c256",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
