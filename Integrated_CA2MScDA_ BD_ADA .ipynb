{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dfc4084a",
   "metadata": {},
   "source": [
    "## <font color='#0000FF'>MSC_DA_CA2 - INTEGRATEDCA2<font color='#1ABC9D'>\n",
    "### <font color='#'>**Advanced Data Analytics  & Big Data Storage and Processing**\n",
    "### <font color='#1ABC9C'>**Lecturer(s): David McQuaid and Muhammad Iqbal**\n",
    "------\n",
    "<font color='#1ABC9C'>**Student name / ID** // Rosilene Francisca da Silva - 2021090"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ad9063",
   "metadata": {},
   "source": [
    "### Data Integration and Preprocessing: Leveraging Apache Spark for Populating MySQL Databases with Large Datasets.\n",
    "\n",
    "### Configure Apache Spark\n",
    "Start Spark Session:\n",
    "Set up PySpark, including necessary packages for MySQL connections."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c80ae7b",
   "metadata": {},
   "source": [
    "\"The dataset used in this analysis, ProjectTweets.csv, was provided by Professor [McQuaid] via the Moodle [https://moodle.cct.ie/mod/assign/view.php?id=44089]course [MSc in Data Analytics] CCT College Dublin on 18 April 2024.\"\n",
    "\n",
    "The dataset is guaranteed to have an organised schema, dependable transactional integrity, strong query capabilities, and simple integration with Apache Spark by selecting MySQL over a NoSQL database. All of these advantages combine to make MySQL the superior option for this particular task of populating the information and doing further analytical processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c121d9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c45c7de2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/11 19:12:25 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "24/05/11 19:12:25 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf, SparkContext\n",
    "import os\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Database Comparative Analysis\") \\\n",
    "    .config(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\") \\\n",
    "    .config(\"spark.driver.memory\", \"8g\") \\\n",
    "    .config(\"spark.executor.memory\", \"8g\") \\\n",
    "    .config(\"spark.executor.cores\", \"4\") \\\n",
    "    .config(\"spark.dynamicAllocation.enabled\", \"true\") \\\n",
    "    .config(\"spark.dynamicAllocation.minExecutors\", \"2\") \\\n",
    "    .config(\"spark.dynamicAllocation.maxExecutors\", \"100\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"1000\") \\\n",
    "    .config(\"spark.jars\", \"/home/hduser/Downloads/mysql-connector-j-8.0.33.jar\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1cb9ec26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.0.2.15:4042\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.4.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Database Comparative Analysis</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x732653fa65c0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17c0369c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10.9.7\n"
     ]
    }
   ],
   "source": [
    "import py4j\n",
    "print(py4j.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24367509",
   "metadata": {},
   "source": [
    "Rationale:\n",
    "This PySpark setup is designed to process data during a database comparison in an effective and scalable manner. With the application name \"Database Comparative Analysis,\" the SparkSession is created, and backward compatibility with timestamp formats is guaranteed by the legacy time parser policy. In order to efficiently handle huge data volumes, the memory configurations for the executors and driver are set to 8 GB each. Moreover, having 4 cores per executor optimises parallel processing.\n",
    "\n",
    "By dynamically allocating resources according to workload requirements, the cluster can grow from a minimum of 2 executors to a maximum of 100. Large dataset data shuffling speed is enhanced by increasing the shuffle divisions to 1000, which reduces execution time. \n",
    "\n",
    "Furthermore, smooth connectivity between Spark and MySQL for direct data intake and analysis is ensured by including the MySQL JDBC connector jar. This setup offers a scalable and highly effective environment appropriate for large-scale database comparison analysis projects."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d2c919",
   "metadata": {},
   "source": [
    "#### Before attempting to read the data, let's test reading data or just establish a connection to identify if has any issue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "38cbc2a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 0:>                                                          (0 + 0) / 1]\r",
      "\r",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-------------------+------------+-------------+--------------------+\n",
      "| id|   user_id|               date|query_status|  user_handle|          tweet_text|\n",
      "+---+----------+-------------------+------------+-------------+--------------------+\n",
      "|  1|1467810672|2009-04-06 22:19:49|    NO_QUERY|scotthamilton|is upset that he ...|\n",
      "+---+----------+-------------------+------------+-------------+--------------------+\n",
      "only showing top 1 row\n",
      "\n",
      "Connection established successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Test loading a simple query to ensure connectivity\n",
    "try:\n",
    "    jdbc_df = spark.read.format(\"jdbc\").option(\"url\", \"jdbc:mysql://localhost/twitter_data\").option(\n",
    "        \"driver\", \"com.mysql.cj.jdbc.Driver\").option(\"dbtable\", \"tweet_details\").option(\n",
    "        \"user\", \"root\").option(\"password\", \"password\").load()\n",
    "    jdbc_df.show(1)\n",
    "    print(\"Connection established successfully.\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(\"An error occurred:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bfa16e9",
   "metadata": {},
   "source": [
    "### Load the data from MySQL into a new DataFrame\n",
    "The Database `twitter_data` was created on MySQL following the `tweet_details`table.  \n",
    "Also defined and renamed the column names as \"id\", \"user_id\", \"date\", \"query_status\", \"user_handle\", and \"tweet_text\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2774ce37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 1:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-------------------+------------+-------------+--------------------+\n",
      "| id|   user_id|               date|query_status|  user_handle|          tweet_text|\n",
      "+---+----------+-------------------+------------+-------------+--------------------+\n",
      "|  1|1467810672|2009-04-06 22:19:49|    NO_QUERY|scotthamilton|is upset that he ...|\n",
      "|  2|1467810917|2009-04-06 22:19:53|    NO_QUERY|     mattycus|@Kenichan I dived...|\n",
      "+---+----------+-------------------+------------+-------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Load the data from MySQL into a new DataFrame\n",
    "mysql_df = spark.read.format(\"jdbc\").options(\n",
    "    url=\"jdbc:mysql://localhost/twitter_data\",\n",
    "    driver=\"com.mysql.cj.jdbc.Driver\",\n",
    "    dbtable=\"tweet_details\",\n",
    "    user=\"root\",\n",
    "    password=\"password\"\n",
    ").load()\n",
    "\n",
    "# Show the first few rows of the DataFrame to verify the data\n",
    "mysql_df.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ac2d93",
   "metadata": {},
   "source": [
    "This code loaded the data from the tweet_details table in MySQL into a new DataFrame called mysql_df. As the data was displayed correctly, it indicates that the data was successfully read to MySQL from PySpark."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c390b498",
   "metadata": {},
   "source": [
    "#### Verify the DataFrame\n",
    "After load the dataset, it’s a good idea to check the type and first few rows of the DataFrame to ensure that everything is loaded correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "91605414",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- user_id: long (nullable = true)\n",
      " |-- date: timestamp (nullable = true)\n",
      " |-- query_status: string (nullable = true)\n",
      " |-- user_handle: string (nullable = true)\n",
      " |-- tweet_text: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 2:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-------------------+------------+-------------+--------------------+\n",
      "| id|   user_id|               date|query_status|  user_handle|          tweet_text|\n",
      "+---+----------+-------------------+------------+-------------+--------------------+\n",
      "|  1|1467810672|2009-04-06 22:19:49|    NO_QUERY|scotthamilton|is upset that he ...|\n",
      "|  2|1467810917|2009-04-06 22:19:53|    NO_QUERY|     mattycus|@Kenichan I dived...|\n",
      "|  3|1467811184|2009-04-06 22:19:57|    NO_QUERY|      ElleCTF|my whole body fee...|\n",
      "|  4|1467811193|2009-04-06 22:19:57|    NO_QUERY|       Karoli|@nationwideclass ...|\n",
      "|  5|1467811372|2009-04-06 22:20:00|    NO_QUERY|     joy_wolf|@Kwesidei not the...|\n",
      "+---+----------+-------------------+------------+-------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Print DataFrame schema\n",
    "mysql_df.printSchema()\n",
    "\n",
    "mysql_df.show(5, truncate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba690cd",
   "metadata": {},
   "source": [
    "#### Dataframe Information\n",
    "Show the number of rows and columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f3050bad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 3:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 1599999, Number of columns: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "num_rows = mysql_df.count()\n",
    "num_columns = len(mysql_df.columns)\n",
    "print(f\"Number of rows: {num_rows}, Number of columns: {num_columns}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545bb96e",
   "metadata": {},
   "source": [
    "With 1,599,999 rows and 7 columns overall, the dataset has a sizable amount of data that may be thoroughly examined. Each row contains particular information possibly connected to tweets, with seven columns reflecting different dataset aspects. These features include numerical and categorical data. This dataset's extensive structure makes it possible to perform significant exploratory data analysis (EDA) and gain important insights about user behaviour and data patterns. It also makes it possible to perform more complex analysis or machine learning activities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618888f1",
   "metadata": {},
   "source": [
    "### Data Pre-Processing in PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "518a2ddd",
   "metadata": {},
   "source": [
    "#### Checking for missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "726d51d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 21:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No missing values found in any column.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Initialize a flag to track whether missing values are found\n",
    "from pyspark.sql.functions import col\n",
    "missing_values_found = False\n",
    "\n",
    "for column in mysql_df.columns:\n",
    "    missing_count = mysql_df.filter(col(column).isNull() | (col(column) == '')).count()\n",
    "    if missing_count > 0:\n",
    "        print(f\"Column {column} has {missing_count} missing values\")\n",
    "        missing_values_found = True\n",
    "\n",
    "# Check the flag after checking all columns, and print a message if no missing values were found\n",
    "if not missing_values_found:\n",
    "    print(\"No missing values found in any column.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c76c77",
   "metadata": {},
   "source": [
    "Based on output the dataset there is no missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "81b9be9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('id', 'int'),\n",
       " ('user_id', 'bigint'),\n",
       " ('date', 'timestamp'),\n",
       " ('query_status', 'string'),\n",
       " ('user_handle', 'string'),\n",
       " ('tweet_text', 'string')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Displaying dtypes of columns\n",
    "mysql_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f01061",
   "metadata": {},
   "source": [
    "From the output the DataFrame schema, the columns `id` and `user_id` are numerical. \n",
    "The column `date`  has a type of timestamp, meaning it's a timestamp (or datetime) field, columns like `query_status` `user_handle`, and `tweet_text` has a type of string, meaning it's a textual field (or varchar in SQL)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bcf616f",
   "metadata": {},
   "source": [
    "#### Displaying some columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f7772b7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 24:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|user_id   |\n",
      "+----------+\n",
      "|1467810672|\n",
      "|1467810917|\n",
      "|1467811184|\n",
      "|1467811193|\n",
      "|1467811372|\n",
      "+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "mysql_df.select('user_id').show(n=5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8d3e4121",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 25:>                                                         (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|date               |\n",
      "+-------------------+\n",
      "|2009-04-06 22:19:49|\n",
      "|2009-04-06 22:19:53|\n",
      "|2009-04-06 22:19:57|\n",
      "|2009-04-06 22:19:57|\n",
      "|2009-04-06 22:20:00|\n",
      "+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mysql_df.select('date').show(n=5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f174f8ba",
   "metadata": {},
   "source": [
    "The output illustrates the temporal character of the data by showing the first five rows of the date column from the dataset. Each entry in the date column is a timestamp indicating when a particular event, such as a tweet, occurred."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575ad16a",
   "metadata": {},
   "source": [
    "#### Using the.agg() method and the aggregation functions min and max to find the earliest and latest dates will print the first and last tweet dates. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "74f2f276",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 26:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First tweet date: 2009-04-06 22:19:49\n",
      "Last tweet date: 2009-06-25 10:28:31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Import Aggregation Functions\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Finding the first and last tweet dates\n",
    "first_last_dates = mysql_df.agg(\n",
    "    F.min(\"date\").alias(\"first_date\"),\n",
    "    F.max(\"date\").alias(\"last_date\")\n",
    ").collect()[0]\n",
    "\n",
    "first_date = first_last_dates[\"first_date\"]\n",
    "last_date = first_last_dates[\"last_date\"]\n",
    "\n",
    "print(f\"First tweet date: {first_date}\")\n",
    "print(f\"Last tweet date: {last_date}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a0ec4c",
   "metadata": {},
   "source": [
    "The earliest and latest timestamps for tweets suggest that the dataset covers the period of April 6, 2009, to June 25, 2009. The last tweet was sent out on June 25, 2009, at 10:28:31, and the first one was sent out on April 6, 2009, at 22:19:49. \n",
    "\n",
    "According to this, the dataset includes tweets from over three months' worth of time, giving it a temporal range that is appropriate for studying tweet patterns and trends during that time. The outcome helps to clarify the historical bounds of the dataset and directs further data analysis techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4271c440",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 29:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------------------------------------------------------------------------------------------------------------+\n",
      "|user_handle  |tweet_text                                                                                                     |\n",
      "+-------------+---------------------------------------------------------------------------------------------------------------+\n",
      "|scotthamilton|is upset that he can't update his Facebook by texting it... and might cry as a result  School today also. Blah!|\n",
      "|mattycus     |@Kenichan I dived many times for the ball. Managed to save 50%  The rest go out of bounds                      |\n",
      "|ElleCTF      |my whole body feels itchy and like its on fire                                                                 |\n",
      "|Karoli       |@nationwideclass no, it's not behaving at all. i'm mad. why am i here? because I can't see you all over there. |\n",
      "|joy_wolf     |@Kwesidei not the whole crew                                                                                   |\n",
      "+-------------+---------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Selecting multiple columns\n",
    "mysql_df.select(['user_handle','tweet_text']).show(n=5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87fceeec",
   "metadata": {},
   "source": [
    "The dataset's first rows provide insight into the `user_handle` and `tweet_text` columns. The user_handle column contains Twitter handles that indicate who wrote each tweet, but the tweet_text column has the actual text content of each tweet. For example, user scotthamilton complains about not being able to update Facebook via text, whereas mattycus describes diving for a ball. Other tweets show personal discomfort (ElleCTF), bewilderment (Karoli), and interactions with other users (joy_wolf). This selection captures various user moods and activities on Twitter within the dataset's duration, emphasising the diversity of content, which ranges from personal updates to social interactions. The truncate=False argument guarantees that the complete content of each tweet is displayed without truncation, providing a preview of the user's tweet interactions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa94228",
   "metadata": {},
   "source": [
    "#### Data Distribution\n",
    "Count distinct values of a categorical column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7e4bda44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 36:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------+\n",
      "|query_status|  count|\n",
      "+------------+-------+\n",
      "|    NO_QUERY|1599999|\n",
      "+------------+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Selecting 'query_status' column\n",
    "mysql_df.groupBy(\"query_status\").count().show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa00cb9",
   "metadata": {},
   "source": [
    "The dataset's `query_status` column. In this instance, the output indicates that the query_status of NO_QUERY applies to all 1,599,999 tweets. This suggests that the query_status column has a consistent value across all rows, indicating that the field was not meaningfully used in the dataset or that none of the tweets were connected to any particular search query. The consistency shows that, absent other columns or contextual information, the column might not be pertinent for more investigation. All things considered, this discovery can direct data analysts to concentrate on other columns for significant insights while taking into account possible drop this column from further studies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "734dd5cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 42:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----+\n",
      "|    user_handle|count|\n",
      "+---------------+-----+\n",
      "|     megan_rice|   15|\n",
      "|         MeghTW|    1|\n",
      "|stranger_danger|   14|\n",
      "|       kyrabeth|    1|\n",
      "|    lovelylivxo|   16|\n",
      "|      tink68113|    1|\n",
      "|     Svalentyna|    1|\n",
      "|     bakerbelle|    1|\n",
      "|  somethingalex|    1|\n",
      "|     sexy_ass_T|    1|\n",
      "+---------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Selecting 'user_handle' column\n",
    "mysql_df.groupBy(\"user_handle\").count().show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a08493",
   "metadata": {},
   "source": [
    "The tweet count of distinct users is calculated by grouping the dataset by the `user_handle` column and counting the number of tweets linked with each user. The first ten rows of the output reveal a unique Twitter user and the number of tweets they have sent. For example, megan_rice has 15 tweets, stranger_danger has 14, and lovelylivxo has 16, yet MeghTW, kyrabeth, so on each have only one tweet. This distribution shows that, while some individuals are frequent tweeters, others have a low presence in the dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27021cd2",
   "metadata": {},
   "source": [
    "#### Summary Statistics\n",
    "Generate summary statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "96573828",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/11 19:16:41 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "[Stage 45:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+--------------------+------------+--------------------+--------------------+\n",
      "|summary|               id|             user_id|query_status|         user_handle|          tweet_text|\n",
      "+-------+-----------------+--------------------+------------+--------------------+--------------------+\n",
      "|  count|          1599999|             1599999|     1599999|             1599999|             1599999|\n",
      "|   mean|         800000.0|1.9988178841753244E9|        null| 4.325887521835714E9|                null|\n",
      "| stddev|461880.0710141109| 1.935756789172917E8|        null|5.162733218454889E10|                null|\n",
      "|    min|                1|          1467810672|    NO_QUERY|        000catnap000|                 ...|\n",
      "|    max|          1599999|          2329205794|    NO_QUERY|          zzzzeus111|ï¿½ï¿½ï¿½ï¿½ï¿½ß§...|\n",
      "+-------+-----------------+--------------------+------------+--------------------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Basic statistics\n",
    "mysql_df.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09029b04",
   "metadata": {},
   "source": [
    "The gives summary statistics for the dataset, shedding light on the distribution and features of each column. \n",
    "\n",
    "With a mean of 800,000.0 and a standard deviation of roughly 461,880, the id column shows a well-distributed set of unique identifiers ranging from 1 to 1,599,999. With a mean of almost 1.998 billion, the user_id column, which represents unique user identifiers, ranges from 1,467,810,672 to 2,329,205,794. All rows in the query_status column have the same value, NO_QUERY. The Twitter `user_handle` column span from 000catnap000 to zzzzeus111, indicating a wide range of users. \n",
    "Finally, a variety of text data are contained in the tweet_text column, including special characters that are evident in the maximum value ï¿½ï¿½ï¿½ï¿½ß§..., which may be indicative of encoding problems. A glimpse of the data distribution is given by the summary, which shows that although most columns are diverse and well-populated, the `query_status` column is uniform, and special characters would need to be cleaned up."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6d283b",
   "metadata": {},
   "source": [
    "#### Correlation Analysis\n",
    "Find correlations between numerical columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bd77775d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numerical columns: ['id', 'user_id']\n"
     ]
    }
   ],
   "source": [
    "# List of numerical columns\n",
    "numerical_columns = [col for col, dtype in mysql_df.dtypes if dtype in ('int', 'bigint', 'double', 'float')]\n",
    "print(f\"Numerical columns: {numerical_columns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e0bdc852",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 48:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+--------------------+\n",
      "|summary|               id|             user_id|\n",
      "+-------+-----------------+--------------------+\n",
      "|  count|          1599999|             1599999|\n",
      "|   mean|         800000.0|1.9988178841753244E9|\n",
      "| stddev|461880.0710141109| 1.935756789172917E8|\n",
      "|    min|                1|          1467810672|\n",
      "|    max|          1599999|          2329205794|\n",
      "+-------+-----------------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Summary statistics only for numerical columns\n",
    "mysql_df.select(numerical_columns).describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "904061ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation between id and user_id: 0.22304937463402058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 54:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation between user_id and id: 0.22304937463402053\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Calculate correlations between numerical columns\n",
    "from pyspark.sql.functions import corr\n",
    "\n",
    "for column1 in numerical_columns:\n",
    "    for column2 in numerical_columns:\n",
    "        if column1 != column2:\n",
    "            correlation = mysql_df.select(corr(column1, column2)).first()[0]\n",
    "            print(f\"Correlation between {column1} and {column2}: {correlation}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e9c51b",
   "metadata": {},
   "source": [
    "There is roughly a 0.223 correlation between `id` and `user_id`. Although there is a linear link between the two columns, it is not very strong, as indicated by the slight positive correlation between id and user_id. This discovery may help direct future research or data modelling by providing insight into the distribution of data and user behaviour."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff64bf48",
   "metadata": {},
   "source": [
    "#### Adding a New Column with the current timestamp & Extract only those tweets that contain the keyword \"summer\" in the tweet_text column.\n",
    "The `inserted_at` column will contain the timestamp indicating when each row was processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cfd07660",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 57:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+--------------------+\n",
      "|               date|          tweet_text|         inserted_at|\n",
      "+-------------------+--------------------+--------------------+\n",
      "|2009-04-06 22:27:00|@jacobsummers Sor...|2024-05-11 19:17:...|\n",
      "|2009-04-06 23:40:34|@kimmyawesome Ohh...|2024-05-11 19:17:...|\n",
      "|2009-04-06 23:48:30|It's official! I'...|2024-05-11 19:17:...|\n",
      "|2009-04-07 00:12:36|ok my TWEET PEEP ...|2024-05-11 19:17:...|\n",
      "|2009-04-07 00:13:15|summer camp or su...|2024-05-11 19:17:...|\n",
      "|2009-04-07 00:34:43|Downy weather  Wh...|2024-05-11 19:17:...|\n",
      "|2009-04-07 00:35:12|Craaaaap. My Macb...|2024-05-11 19:17:...|\n",
      "|2009-04-07 00:41:46|@dadi_iyal and yo...|2024-05-11 19:17:...|\n",
      "|2009-04-07 01:57:26|@meatrack no more...|2024-05-11 19:17:...|\n",
      "|2009-04-07 01:57:35|searching for a j...|2024-05-11 19:17:...|\n",
      "+-------------------+--------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Filtering tweets containing a specific keyword 'summer'\n",
    "filtered_df = mysql_df.filter(mysql_df.tweet_text.contains(\"summer\"))\n",
    "\n",
    "# Add a current timestamp column for the insert time\n",
    "from pyspark.sql.functions import current_timestamp\n",
    "\n",
    "final_df = filtered_df.withColumn(\"inserted_at\", current_timestamp())\n",
    "\n",
    "# Select only the desired columns and print it\n",
    "final_df.select(\"date\", \"tweet_text\", \"inserted_at\").show(10, truncate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e161afb1",
   "metadata": {},
   "source": [
    "After filtering the dataset, this result displays the first ten rows of tweets that contain the keyword \"summer\". Each tweet is shown with its original timestamp (date), the tweet's text (tweet_text), and a new column (inserted_at) that shows the current timestamp when this information was processed. The date column providing information on the temporal distribution of tweets mentioning \"summer.\" For example, one user is undecided between \"summer camp or summer school,\" while another tweets about \"Downy Weather.\" The `inserted_at` column indicates that the data was processed, distinguishing between the original tweet timestamps and the time of processing. \n",
    "\n",
    "This allows for the tracking of when the filtered data was curated. Overall, this result displays a wide range of attitudes and interests relating to \"summer,\" from plans to technical concerns, providing useful insights into user interactions about this topic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad0ca1a",
   "metadata": {},
   "source": [
    "#### Word Frequency Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b1086f72",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 60:>                                                         (0 + 4) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "|word|  count|\n",
      "+----+-------+\n",
      "|    |1184159|\n",
      "|  to| 552961|\n",
      "|   I| 496616|\n",
      "| the| 487501|\n",
      "|   a| 366211|\n",
      "|  my| 280025|\n",
      "| and| 275263|\n",
      "|   i| 249976|\n",
      "|  is| 217692|\n",
      "| you| 213871|\n",
      "+----+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Frequency of words in the \"tweet_text\" column\n",
    "from pyspark.sql.functions import explode, split\n",
    "mysql_df.withColumn(\"word\", explode(split(mysql_df[\"tweet_text\"], \"\\s+\"))).groupBy(\"word\").count().orderBy(\"count\", ascending=False).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a47464",
   "metadata": {},
   "source": [
    "To analyze text data, paying particular attention to word frequency in the \"tweet_text\" column. The DataFrame is then grouped according to the \"word\" column, and the count() method is used to determine how many times each word appears. The most common terms are then highlighted by sorting the results in descending order using the \"count\" column. Lastly, the top ten outcomes are shown. The output displays the word counts. Common English terms like \"to,\" \"I,\" and \"the\" are followed by the most often occurring word, 1,184,159 times (blank spaces, probably from many consecutive spaces or formatting errors in tweets).\n",
    "\n",
    "Rationale: Understanding the dataset's typical word usage, identifying any data cleaning problems (such as the large number of blanks), and using the results as a foundation for additional text analysis tasks like sentiment analysis or theme modelling are all made possible by this analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e35521db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['id', 'user_id', 'date', 'query_status', 'user_handle', 'tweet_text']\n"
     ]
    }
   ],
   "source": [
    "# Print current DataFrame columns\n",
    "print(mysql_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "973eb182",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('id', 'int'),\n",
       " ('user_id', 'bigint'),\n",
       " ('date', 'timestamp'),\n",
       " ('query_status', 'string'),\n",
       " ('user_handle', 'string'),\n",
       " ('tweet_text', 'string')]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Displaying dtypes of columns\n",
    "mysql_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8bfdc45",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "### Converting the dataset from MySQL to Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3163a1ce",
   "metadata": {},
   "source": [
    "#### Before converting the dataset to pandas, drop some unnecessary columns for further analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a851d6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Droping'id','query_status' columns\n",
    "mysql_df = mysql_df.drop('id','query_status')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e34ec8a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 61:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+-------------+--------------------+\n",
      "|   user_id|               date|  user_handle|          tweet_text|\n",
      "+----------+-------------------+-------------+--------------------+\n",
      "|1467810672|2009-04-06 22:19:49|scotthamilton|is upset that he ...|\n",
      "|1467810917|2009-04-06 22:19:53|     mattycus|@Kenichan I dived...|\n",
      "+----------+-------------------+-------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "mysql_df.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb86ca02",
   "metadata": {},
   "source": [
    "#### Checking for missing Values again after dropped some columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "74f1e628",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 62:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+-----------+----------+\n",
      "|user_id|date|user_handle|tweet_text|\n",
      "+-------+----+-----------+----------+\n",
      "|      0|   0|          0|         0|\n",
      "+-------+----+-----------+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, sum\n",
    "\n",
    "null_count = mysql_df.select([sum(col(c).isNull().cast(\"int\")).alias(c) for c in mysql_df.columns])\n",
    "null_count.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c191f1",
   "metadata": {},
   "source": [
    "#### Collect the Dataset to Pandas \n",
    "Objective: Gather all rows from the Spark DataFrame (mysql_df) into a list of rows.\n",
    "\n",
    "Functionality:\n",
    ".collect(): Fetches all data rows from the distributed environment into the driver node as a list of Row objects.\n",
    "Rationale: Because this operation requires sufficient memory and is suitable for relatively smaller datasets due to its memory-intensive nature, the memory configurations for the executors and driver were set from 4 GB to 8 GB each, and the configuration bellow on Spark in the beginning was changed, and unnecessary columns were removed before running the process.    \n",
    "    .config(\"spark.executor.cores\", \"4\") \\\n",
    "    .config(\"spark.dynamicAllocation.enabled\", \"true\") \\\n",
    "    .config(\"spark.dynamicAllocation.minExecutors\", \"2\") \\\n",
    "    .config(\"spark.dynamicAllocation.maxExecutors\", \"100\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"1000\") \\ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9280b11b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Collect the Dataset to Pandas\n",
    "new_df = mysql_df.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f055c8e1",
   "metadata": {},
   "source": [
    "#### Create Pandas DataFrame from MySQL\n",
    "Objective: Convert the list of rows into a Pandas DataFrame for further analysis.\n",
    "\n",
    "Functionality: pd.DataFrame(new_df): Initializes a Pandas DataFrame from the list of rows.\n",
    "twitter_df.columns = mysql_df.columns: Assigns the original column names from the Spark DataFrame (mysql_df) to the Pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b5bcdb0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>date</th>\n",
       "      <th>user_handle</th>\n",
       "      <th>tweet_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1467810672</td>\n",
       "      <td>2009-04-06 22:19:49</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1467810917</td>\n",
       "      <td>2009-04-06 22:19:53</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1467811184</td>\n",
       "      <td>2009-04-06 22:19:57</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1467811193</td>\n",
       "      <td>2009-04-06 22:19:57</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1467811372</td>\n",
       "      <td>2009-04-06 22:20:00</td>\n",
       "      <td>joy_wolf</td>\n",
       "      <td>@Kwesidei not the whole crew</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      user_id                date    user_handle  \\\n",
       "0  1467810672 2009-04-06 22:19:49  scotthamilton   \n",
       "1  1467810917 2009-04-06 22:19:53       mattycus   \n",
       "2  1467811184 2009-04-06 22:19:57        ElleCTF   \n",
       "3  1467811193 2009-04-06 22:19:57         Karoli   \n",
       "4  1467811372 2009-04-06 22:20:00       joy_wolf   \n",
       "\n",
       "                                          tweet_text  \n",
       "0  is upset that he can't update his Facebook by ...  \n",
       "1  @Kenichan I dived many times for the ball. Man...  \n",
       "2    my whole body feels itchy and like its on fire   \n",
       "3  @nationwideclass no, it's not behaving at all....  \n",
       "4                      @Kwesidei not the whole crew   "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create Pandas DataFrame from MySQL\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "twitter_df = pd.DataFrame(new_df)\n",
    "twitter_df.columns =  mysql_df.columns\n",
    "twitter_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3d3b98e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1599999, 4)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778d0e50",
   "metadata": {},
   "source": [
    "The techniques was applyed to collect a large dataset from a Spark DataFrame (mysql_df) to a Pandas DataFrame (twitter_df) for further analysis. .collect() gathers all rows from the Spark DataFrame into a list of Row objects, which are then transformed to a Pandas DataFrame. The resulting twitter_df has 1,599,999 rows and 4 columns, indicating that the dataset is large enough to be explored and manipulated with Pandas. However, due to the scale of the dataset, attention should be given with memory utilisation, as Pandas operations might be more memory-intensive than Spark DataFrames."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989a2768",
   "metadata": {},
   "source": [
    "### Save the Dataset to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "97e73e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the Pandas DataFrame to a CSV file\n",
    "twitter_df.to_csv('twitter_df.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c9e9ae85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 401M\r\n",
      "-rw-r--r-- 1 hduser hadoopgroup  53K May 11 19:17 'Integrated_CA2MScDA_ BD_ADA -BDS1.ipynb'\r\n",
      "-rw-r--r-- 1 hduser hadoopgroup 104K May 11 19:18 'Integrated_CA2MScDA_ BD_ADA .ipynb'\r\n",
      "drwxr-xr-x 4 hduser hadoopgroup 4.0K May  6 10:02  MSCCA12023V2\r\n",
      "-rw-r--r-- 1 hduser hadoopgroup 2.4M May  5 15:20  mysql-connector-j-8.0.33.jar\r\n",
      "-rw-r--r-- 1 hduser hadoopgroup 219M Apr 28 09:50  ProjectTweets.csv\r\n",
      "-rw-r--r-- 1 hduser hadoopgroup   32 Apr 25 09:23  README.md\r\n",
      "-rw-r--r-- 1 hduser hadoopgroup 180M May 11 19:20  twitter_df.csv\r\n"
     ]
    }
   ],
   "source": [
    "!ls -lh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e61b14",
   "metadata": {},
   "source": [
    "The collected  DataFrame (twitter_df) is saved to a CSV file named twitter_df.csv via the to_csv() method. The index=False argument ensures that the index column does not appear in the output. This method facilitates data export, allowing the dataset to be shared, analysed, or preserved in CSV format. The generated CSV file has 1,599,999 rows and four columns (user_id, date, user_handle, and tweet_text), offering a complete and portable snapshot of Twitter data for further analysis.\n",
    "\n",
    "The command `!ls -lh` was applyed to list all files in the current directory with detailed information like size, permissions, owner, and timestamps in a readable format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb55a4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3caa4a7e",
   "metadata": {},
   "source": [
    "## Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bdf4ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip uninstall matplotlib -y\n",
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f02ae83",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Importing the libraries.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "from matplotlib import cm\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2973bced",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Command to display all columns in the file.\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9546bb7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the Dataset \n",
    "tweets_df = pd.read_csv('ProjectTweets.csv')\n",
    "tweets_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d947cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set column's names\n",
    "tweets_df.columns = ['index', 'user_id', 'date', 'query_status', 'user_handle', 'tweet_text']\n",
    "tweets_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d7a185",
   "metadata": {},
   "source": [
    "After load the CSV file \"ProjectTweets.csv\" was noted that it has no headers. Renaming columns in a DataFrame makes the code cleaner and easier to comprehend and maintain. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f03763",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09841a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1819b85",
   "metadata": {},
   "source": [
    "Based on the first basic details, the dataset appears to be a collection of tweets, commonly can be use in sentiment analysis or other natural language processing tasks. \n",
    "\n",
    "Dataset Size and Integrity:\n",
    "The dataset contains approximately 1.6 million entries (1599999 entries, indexed from 0 to 1599998).\n",
    "Each column has the same number of non-null entries (1599999), indicating there are no missing values in any of the columns.\n",
    "Memory Usage: The dataframe is consuming approximately 73.2 MB of memory, which is relevant for understanding the computation resources needed to process this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7935cd07",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4577a43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('length of data is', len(tweets_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcbf05b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df.isnull().values.any()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d267c14",
   "metadata": {},
   "source": [
    "#### Filtering Data\n",
    "Filter rows based on some criteria. For example, to see tweets that have no query status:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c498121",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_query_tweets = tweets_df[tweets_df['query_status'] == 'NO_QUERY']\n",
    "print(no_query_tweets.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe13cbd4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tweets_df.query_status.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2d2528",
   "metadata": {},
   "source": [
    "The feature `query_status` shows whether any specific query key phrase has been used to collect the tweets; nonetheless, 100% of the entries in this column have the value \"NO_QUERY.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91173610",
   "metadata": {},
   "source": [
    "#### Display the Unique Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594f8194",
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in tweets_df.columns:\n",
    "    num_unique_values = tweets_df[column].nunique()\n",
    "    print(f\"Number of unique values in '{column}': {num_unique_values}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4786a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_unique_values(data_frame):\n",
    "    unique_dataframe = pd.DataFrame()\n",
    "    unique_dataframe['Features'] = data_frame.columns\n",
    "    uniques = []\n",
    "    for col in data_frame.columns:\n",
    "        u = data_frame[col].nunique()\n",
    "        uniques.append(u)\n",
    "    unique_dataframe['Uniques'] = uniques\n",
    "    return unique_dataframe\n",
    "\n",
    "unidf = return_unique_values(tweets_df)\n",
    "print(unidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe0a79b4",
   "metadata": {},
   "source": [
    "##### To begin with, dropping the columns that are unnecessary for the particular goal of sentiment analysis.The `index` and `query_status`, columns in this instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea583302",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping the 'index' and 'query_status' columns\n",
    "tweets_df.drop(columns=['index', 'query_status'], inplace=True)\n",
    "\n",
    "# Display the DataFrame to confirm changes\n",
    "tweets_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8223923",
   "metadata": {},
   "source": [
    "#### DateTime Parsing - Convert Date Format\n",
    "The `date` column contains date and time information as a string, let's to convert it into a DateTime for easier manipulation and to facilitate time series operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce7d201",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df['date'] = pd.to_datetime(tweets_df['date'], errors='coerce', format='%a %b %d %H:%M:%S PDT %Y')\n",
    "tweets_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fbae456",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tweets_df['date'].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eab0c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd07107",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d46ae3",
   "metadata": {},
   "source": [
    "The summary statistics for the user_id and date columns provide many insights into the user's activity and timing characteristics during the data collecting period. The user_id has a mean of around 1.998 billion, indicating a wide range of values, potentially due to a big user base or a wide encoding range for user identification. The difference between the minimum (1.467811e+09) and maximum (2.329206e+09) values, with a standard deviation of around 193.6 million, indicates significant variability in user ID assignments, which could be due to different periods of user registration or system changes affecting ID allocation. The date column ranges from April 6, 2009 to June 25, 2009, with the median occurring on June 2, 2009. \n",
    "\n",
    "The dataset may have captured seasonal user behaviour or events because of its temporal distribution, which shows that it covers almost three months in late spring to early summer. The mid-June and end-of-May 25th and 75th percentiles, respectively, point to a concentration of recordings around these dates, which may correspond with particular events or promotions that encourage user participation during these times."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0438625",
   "metadata": {},
   "source": [
    "#### Text Analysis\n",
    "Analyzing the tweet texts, let's counting the number of words in each tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327335a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the length of each tweet and the word count\n",
    "tweets_df['text_length'] = tweets_df['tweet_text'].apply(len)\n",
    "tweets_df['word_count'] = tweets_df['tweet_text'].apply(lambda x: len(x.split()))\n",
    "\n",
    "# Display these new metrics\n",
    "print(tweets_df[['tweet_text', 'text_length', 'word_count']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ea03cd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tweets_df['word_count'] = tweets_df['tweet_text'].apply(lambda x: len(str(x).split()))\n",
    "tweets_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2423af57",
   "metadata": {},
   "source": [
    "#### Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d2ad213",
   "metadata": {},
   "source": [
    "Using a simple library like TextBlob, to get a rough idea of the sentiment of each tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae1cf47",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1262292a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "# Calculate sentiment polarity\n",
    "tweets_df['sentiment'] = tweets_df['tweet_text'].apply(lambda text: TextBlob(text).sentiment.polarity)\n",
    "\n",
    "# Display tweets with their sentiment\n",
    "print(tweets_df[['tweet_text', 'sentiment']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c098cd0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "45ebb2ff",
   "metadata": {},
   "source": [
    "### Visualization\n",
    "Visualizing aspects of the data can further aid in understanding the distribution and relationships."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45864ab4",
   "metadata": {},
   "source": [
    "#### Time Series Plot of Tweet Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee390eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plotting the number of tweets over time\n",
    "tweets_df.set_index('date').resample('S').size().plot()\n",
    "plt.title('Tweet Frequency Over Time')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Number of Tweets')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ababcab9",
   "metadata": {},
   "source": [
    "#### Histogram of Text Length and Word Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1530eda3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df['text_length'].plot(kind='hist', title='Distribution of Tweet Text Length', bins=20, alpha=0.7)\n",
    "plt.xlabel('Text Length')\n",
    "plt.show()\n",
    "\n",
    "tweets_df['word_count'].plot(kind='hist', title='Distribution of Tweet Word Count', bins=20, alpha=0.7)\n",
    "plt.xlabel('Word Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce5cb14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4bb1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Convert to Pandas DataFrame\n",
    "tweet_data = jdbc_df.limit(1000).toPandas()\n",
    "\n",
    "# Example visualization: Histogram\n",
    "tweet_data['tweet_length'] = tweet_data['tweet_text'].apply(len)\n",
    "tweet_data['tweet_length'].hist(bins=20)\n",
    "plt.xlabel(\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c20102",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e9796d76",
   "metadata": {},
   "source": [
    "#### Distributions\n",
    "Explore the distribution of key variables using histograms or box plots to understand their central tendencies and spread.\n",
    "\n",
    "#### Histograms: Useful for visualizing the distribution of numerical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eecf7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_ids = tweets_df['user_id']\n",
    "\n",
    "# Calculate histogram data\n",
    "counts, bin_edges = np.histogram(user_ids, bins=10)\n",
    "\n",
    "# Create a histogram\n",
    "plt.figure(figsize=(10, 6))  # Make it large enough for clarity\n",
    "plt.bar(bin_edges[:-1], counts, width=np.diff(bin_edges), edgecolor='black', align='edge', color='skyblue')\n",
    "plt.title('Distribution of User IDs')\n",
    "plt.xlabel('User ID Bins')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True)  \n",
    "\n",
    "# Remove all unnecessary tick marks and frame pieces\n",
    "plt.tick_params(top=False, right=False, left=True, bottom=True, labelleft=True, labelbottom=True)\n",
    "for spine in plt.gca().spines.values():\n",
    "    if spine.spine_type not in ['bottom', 'left']:  \n",
    "        spine.set_visible(False)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4567fcf",
   "metadata": {},
   "source": [
    "#### Box Plots: Good for detecting outliers and understanding the distribution's quartiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f730f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a box plot\n",
    "plt.figure(figsize=(8, 6)) \n",
    "plt.boxplot(user_ids, vert=False, widths=0.7, patch_artist=True, flierprops={'marker':'o', 'color':'red', 'markersize':5})\n",
    "plt.title('User ID Distribution')\n",
    "plt.xlabel('User IDs')\n",
    "plt.yticks([])  \n",
    "\n",
    "# Enhance data-ink ratio\n",
    "plt.gca().spines['top'].set_visible(False)\n",
    "plt.gca().spines['right'].set_visible(False)\n",
    "plt.gca().spines['left'].set_visible(False)  # Remove left spine if y-axis ticks are not informative\n",
    "plt.grid(True, linestyle='--', which='major', color='gray', alpha=0.5)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccce74d7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aaa3d79b",
   "metadata": {},
   "source": [
    "#### Plot histograms and density plots of user_id to understand user engagement and presence. \n",
    "Because the histogram helps to visualize how active users are in terms of the number of tweets they post. And the density plot provides a clear view of the distribution's shape, highlighting the typical user activity levels without the binning process of a histogram.\n",
    "\n",
    "First, set up correctly for visualising user activity using user_id and analysing the frequency of user interactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4103b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the frequency of each user_id\n",
    "user_activity = tweets_df['user_id'].value_counts()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a histogram of user activity\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(user_activity.values, bins=30, color='skyblue', edgecolor='black')\n",
    "plt.title('Histogram of User Activity')\n",
    "plt.xlabel('Number of Tweets per User')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True, which='both', linestyle='--', linewidth=0.5, alpha=0.7)  # Minimal grid use\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3e09d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# Create a density plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.kdeplot(user_activity.values, shade=True, color='blue', alpha=0.7)\n",
    "plt.title('Density Plot of User Activity')\n",
    "plt.xlabel('Number of Tweets per User')\n",
    "plt.ylabel('Density')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "780943f4",
   "metadata": {},
   "source": [
    "#### Pair Plots\n",
    "Use pair plots to visualise the relationships and distributions of each pair of variables in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a241c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a pair plot with a clean and minimalist design\n",
    "sns.set(style=\"white\")  # Sets the style of the plot to a simple white background\n",
    "pairplot = sns.pairplot(tweets_df, diag_kind='kde', plot_kws={'alpha': 0.6, 's': 80, \n",
    "                                                              'edgecolor': 'k'}, corner=True)\n",
    "for i in range(len(pairplot.axes)):\n",
    "    for j in range(len(pairplot.axes)):\n",
    "        if i != j:\n",
    "            pairplot.axes[i][j].set_visible(False)\n",
    "            if i == j:\n",
    "                pairplot.axes[i][j].set_ylabel('Density')\n",
    "\n",
    "# Add titles and labels\n",
    "plt.subplots_adjust(top=0.9)\n",
    "pairplot.fig.suptitle('Pairwise Plots of User Metrics', fontsize=16)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54109af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56e9e85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2ff77333",
   "metadata": {},
   "source": [
    "#### Correlation Matrix\n",
    "Investigate the correlations between numerical variables to better understand the links between the fields in the dataset.\n",
    "Creating a correlation matrix is an excellent way to visually and quantitatively investigate the correlations between numerical variables in your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581bd546",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Compute the correlation matrix\n",
    "corr_matrix = tweets_df.corr()\n",
    "\n",
    "# Print the correlation matrix\n",
    "print(corr_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4202fd4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a heatmap from the correlation matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(corr_matrix, annot=True, fmt=\".2f\", cmap='coolwarm', cbar=True, square=True, linewidths=.5)\n",
    "\n",
    "# Enhancing the heatmap\n",
    "plt.title('Correlation Matrix of Variables')\n",
    "plt.yticks(rotation=0)  # Rotate y-labels for better readability\n",
    "plt.xticks(rotation=90)  # Rotate x-labels for better readability\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87a6753",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bb61fa8c",
   "metadata": {},
   "source": [
    "### Text Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "067cda35",
   "metadata": {},
   "source": [
    "#### Sentiment Analysis: \n",
    "Planing to forecast sentiment trends, let's first need to compute the sentiment scores of the tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90724845",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "tweets_df['sentiment'] = tweets_df['tweet_text'].apply(lambda x: TextBlob(x).sentiment.polarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa6c69a",
   "metadata": {},
   "source": [
    "#### Aggregate Sentiment Over Time: \n",
    "Resample the sentiment data to daily, and calculate mean sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b58fa23",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_sentiment = tweets_df['sentiment'].resample('D').mean()\n",
    "daily_sentiment.plot(title='Daily Sentiment Score')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Sentiment Score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1206e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c82389aa",
   "metadata": {},
   "source": [
    "### Time Series Analysis and Forecasting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0271ccfb",
   "metadata": {},
   "source": [
    "#### Set Date as Index: For time series analysis, it's useful to have the date as the DataFrame index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8070cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df.set_index('date', inplace=True)\n",
    "tweets_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5330e7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3752904c",
   "metadata": {},
   "source": [
    "#### Trend Analysis: \n",
    "Ploting time series of counts of tweets per day to see if there's any visible trend or seasonality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d3811f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df.resample('D').size().plot(title='Daily Tweets')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Number of tweets')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3537c6",
   "metadata": {},
   "source": [
    "#### Seasonality Check: \n",
    "Use seasonal decomposition to observe inherent seasonality in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27020dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install statsmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0dbd6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "result = seasonal_decompose(tweets_df.resample('D').size(), model='additive')\n",
    "result.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7efad0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0123a37a",
   "metadata": {},
   "source": [
    "### Correlation and Causation Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36891b21",
   "metadata": {},
   "source": [
    "#### Stationarity Check: \n",
    "Ensure the time series data is stationary, as this is a requirement for models like ARIMA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d44859",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import adfuller\n",
    "test_result = adfuller(daily_sentiment.dropna())\n",
    "print('ADF Statistic: %f' % test_result[0])\n",
    "print('p-value: %f' % test_result[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2f9662",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4fb81403",
   "metadata": {},
   "source": [
    "### Data Splitting: \n",
    "Decide on the training and testing periods for the models. Typically, the latter part of the data is held out for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fdccfc7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9fde5b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6b9706e1",
   "metadata": {},
   "source": [
    "### Apache Spark\n",
    "\n",
    "Setup the Notebook Environment.\n",
    "\n",
    "Import the required libraries and set up the Spark session to interface with Cassandra and handle data processing needs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9554a590",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf, SparkContext\n",
    "import os\n",
    "\n",
    "# Set the SPARK_LOCAL_IP environment variable if necessary\n",
    "# os.environ['SPARK_LOCAL_IP'] = '10.0.2.15' \n",
    "\n",
    "# Initialize a Spark session and Configure Spark to interact with Cassandra\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ProjectTweets.csv\") \\\n",
    "    .config(\"spark.master\", \"local\") \\\n",
    "    .config(\"spark.ui.port\", \"4050\") \\\n",
    "    .config(\"spark.cassandra.connection.host\", \"localhost\") \\\n",
    "    .config(\"spark.cassandra.connection.port\", \"9042\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d652472",
   "metadata": {},
   "source": [
    "#### Data Loading and Preliminary Analysis\n",
    "\n",
    "First, load the CSV file \"ProjectTweets.csv\" without assuming it has headers. With specify header=False to tell Spark that the first row should not be treated as headers, and then it will explicitly provide the column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9917a203",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+----------------------------+--------+---------------+-------------------------------------------------------------------------------------------------------------------+\n",
      "|_c0|_c1       |_c2                         |_c3     |_c4            |_c5                                                                                                                |\n",
      "+---+----------+----------------------------+--------+---------------+-------------------------------------------------------------------------------------------------------------------+\n",
      "|0  |1467810369|Mon Apr 06 22:19:45 PDT 2009|NO_QUERY|_TheSpecialOne_|@switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D|\n",
      "|1  |1467810672|Mon Apr 06 22:19:49 PDT 2009|NO_QUERY|scotthamilton  |is upset that he can't update his Facebook by texting it... and might cry as a result  School today also. Blah!    |\n",
      "+---+----------+----------------------------+--------+---------------+-------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tweets_df = spark.read.csv(\"file:///home/hduser/Desktop/INTEGRATEDCA2V2/ProjectTweets.csv\", \n",
    "                                   header=False, inferSchema=True)\n",
    "tweets_df.show(2, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c94a12d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f3c712c8",
   "metadata": {},
   "source": [
    "### Data Cleaning and Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883ed0d9",
   "metadata": {},
   "source": [
    "Because this dataset lacks column titles, column names must be assigned manually when the data is loaded into Spark. This guarantees that the data processing and analysis processes are simple to comprehend and manage. \n",
    "\n",
    "Let's implement it properly,using the `toDF()` method as it given the data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c170cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define column names\n",
    "column_names = [\"id\", \"user_id\", \"date\", \"query_status\", \"user_handle\", \"tweet_text\"]\n",
    "\n",
    "# Rename the columns\n",
    "tweets_df = tweets_df.toDF(*column_names)\n",
    "tweets_df.show(5, truncate=False) # Using truncate=False allows to see the full content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d46c82c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print current DataFrame columns to understand the naming\n",
    "print(tweets_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0821eeaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying dtypes of columns\n",
    "tweets_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359a003c",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(tweets_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58e2788",
   "metadata": {},
   "source": [
    "#### Verify the DataFrame\n",
    "After renaming the columns, it’s a good idea to check the first few rows of the DataFrame to ensure that everything is loaded correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053d45eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print DataFrame schema\n",
    "tweets_df.printSchema()\n",
    "\n",
    "tweets_df.show(5, truncate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e7633d9",
   "metadata": {},
   "source": [
    "Based on the first details, the dataset appears to be a collection of tweets, commonly can be use in sentiment analysis or other natural language processing tasks. \n",
    "\n",
    "The dataset columns like 'id' and 'user_id' employ Integer data types, which are great for unique IDs and indexing. Meanwhile, columns labelled 'date', 'query_status', 'user_handle', and 'tweet_text' are of the String type, which can include a wide range of textual and category information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b83d20c",
   "metadata": {},
   "source": [
    "#### Saving the DataFrame\n",
    "Consider saving this DataFrame in a more effective format, such as Parquet, in case it needs to be reused with the correct column names in subsequent sessions or analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124455c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the DataFrame for future use\n",
    "tweets_df.write.parquet(\"/path/to/save/tweets_df.parquet\", mode=\"overwrite\")\n",
    "\n",
    "# Load it whenever needed\n",
    "tweets_df = spark.read.parquet(\"/path/to/save/tweets_df.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0be799",
   "metadata": {},
   "source": [
    "Saving a DataFrame in a format such as Parquet is extremely useful, particularly when dealing with huge datasets or when the same data will be reused across numerous sessions or projects. Here are some of the primary reasons why saving the DataFrame in a format like Parquet is recommended: Storage and I/O operations are more efficient, with built-in schema preservation and the ability to handle complex data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d29b394",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of records\n",
    "print(f\"Class: {type(tweets_df)}\")\n",
    "print(f\"Total records: {tweets_df.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "758c5810",
   "metadata": {},
   "source": [
    "The dataset contains approximately 1.6 million entries. \n",
    "Scalability of the dataset: The dataset's 1.6 million items make it suitable for training increasingly sophisticated data-driven models, particularly deep learning models used in natural language processing and other machine learning applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740aa10b",
   "metadata": {},
   "source": [
    "#### Checking for Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c36813",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Get number of non-nulls per column by subtracting count of nulls from total entries\n",
    "total_entries = tweets_df.count()\n",
    "for column in tweets_df.columns:\n",
    "    non_nulls = total_entries - tweets_df.filter(tweets_df[column].isNull()).count()\n",
    "    print(f\"Column '{column}' has {non_nulls} non-null entries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5daaee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a flag to track whether missing values are found\n",
    "missing_values_found = False\n",
    "\n",
    "for column in tweets_df.columns:\n",
    "    missing_count = tweets_df.filter(col(column).isNull() | (col(column) == '')).count()\n",
    "    if missing_count > 0:\n",
    "        print(f\"Column {column} has {missing_count} missing values\")\n",
    "        missing_values_found = True\n",
    "\n",
    "# Check the flag after checking all columns, and print a message if no missing values were found\n",
    "if not missing_values_found:\n",
    "    print(\"No missing values found in any column.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35308a88",
   "metadata": {},
   "source": [
    "After checking each column in the dataset for null entries or empty strings, no such missing values were found across all columns, indicating that the dataset is complete and ready for further analysis without the requirement for data cleaning to fill or remove missing entries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3f8c8a",
   "metadata": {},
   "source": [
    "### Data Exploration (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d74f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "print(pyspark.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00a96e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import py4j\n",
    "print(py4j.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d677f605",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4730aa86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.0.2.15:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.4.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Database Comparative Analysis</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7bf59f996290>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d44d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first few rows of the DataFrame\n",
    "print(pandas_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faca392b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics for numerical columns\n",
    "print(pandas_df.describe())\n",
    "\n",
    "# Check for missing values\n",
    "print(pandas_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7528126b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic statistics\n",
    "tweets_df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85a53c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying the values of a particular column\n",
    "tweets_df.select('date').show(n=10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d165f2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting multiple columns\n",
    "tweets_df.select(['user_id','user_handle']).show(n=10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1092977c",
   "metadata": {},
   "source": [
    "### HandySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dedfe4bc",
   "metadata": {},
   "source": [
    "HandySpark is very helpful when moving from Pandas to PySpark since it creates a bridge by giving Spark DataFrames an interface similar to Pandas. Improved statistical analysis tools that are not easily accessible in PySpark, improved visualisation possibilities straight from DataFrames, and simpler, more intuitive data manipulation are all made possible by this connection. With features like stratified sampling, HandySpark also makes managing unbalanced datasets easier and provides improved methods for handling missing data. HandySpark retains the scalable architecture of Spark, even though it adds some overhead when compared to native PySpark. This makes it a viable option for individuals who want to combine the simplicity of Pandas with the capability of distributed computing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b34273",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install handyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e4eec3",
   "metadata": {},
   "source": [
    "#### Distributions\n",
    "Examine the distribution of key variables using histograms or box plots to understand their central tendencies and spread."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4bca374",
   "metadata": {},
   "source": [
    "#### Histograms: Useful for visualizing the distribution of numerical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebff6d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f399c9",
   "metadata": {},
   "source": [
    "Caching can prevent recomputation of the DataFrame from the source data, improving performance for repeated queries and transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47df1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from handyspark import *\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create or get the existing Spark session\n",
    "spark = SparkSession.builder.appName(\"User ID Histogram with HandySpark\").getOrCreate()\n",
    "\n",
    "# Convert your PySpark DataFrame to a HandySpark DataFrame\n",
    "hdf = tweets_df.toHandy()\n",
    "\n",
    "# Using HandySpark to plot a histogram\n",
    "hdf.cols['user_id'].hist(bins=10, figsize=(8, 6), color='blue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46702680",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_data = tweets_df.select(\"user_id\").rdd.flatMap(lambda x: x).histogram(10)\n",
    "pd.DataFrame(list(zip(*numeric_data)), columns=[\"bin\", \"frequency\"]).plot(kind=\"bar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681a3f30",
   "metadata": {},
   "source": [
    "#### Box Plots: Good for detecting outliers and understanding the distribution's quartiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb12b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df.toPandas().boxplot(column=['user_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0edc8e05",
   "metadata": {},
   "source": [
    "#### Pair Plots\n",
    "Use pair plots to visualise the relationships and distributions of each pair of variables in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aab3c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_pd = tweets_df.select(numeric_features).sample(False, 0.1).toPandas()\n",
    "sns.pairplot(sampled_pd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d653e39",
   "metadata": {},
   "source": [
    "#### Correlation Matrix\n",
    "Investigate the correlations between numerical variables to better understand the links between the fields in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97bda79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features = [t[0] for t in tweets_df.dtypes if t[1] == 'int' or t[1] == 'double']\n",
    "sampled_data = tweets_df.select(numeric_features).sample(False, 0.1).toPandas()\n",
    "correlations = sampled_data.corr()\n",
    "sns.heatmap(correlations, annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda8e83a",
   "metadata": {},
   "source": [
    "### Categorical Data Analysis:\n",
    "\n",
    "If the data contains categorical information, use bar or pie charts to visualise the distribution of categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f826717",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df.groupBy(\"some_categorical_column\").count().toPandas().plot(kind='bar', x='some_categorical_column', y='count')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45dfebb4",
   "metadata": {},
   "source": [
    "#### Pie Charts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b4f5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "pie_data = tweets_df.groupBy(\"another_categorical_column\").count().toPandas()\n",
    "pie_data.plot(kind='pie', y='count', labels=pie_data['another_categorical_column'], autopct='%1.1f%%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca965ff0",
   "metadata": {},
   "source": [
    "#### Bar Charts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79fd5ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df.groupBy(\"some_categorical_column\").count().toPandas().plot(kind='bar', x='some_categorical_column', y='count')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b594f492",
   "metadata": {},
   "source": [
    "#### Time Series Analysis\n",
    "For data with a temporal component, plot time series to see trends, cycles, or irregular patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92343ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_series_data = tweets_df.groupBy(\"date_column\").agg({\"numerical_column\": \"mean\"})\n",
    "time_series_data.toPandas().plot(x='date_column', y='mean_numerical_column')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b2eed7",
   "metadata": {},
   "source": [
    "#### Text Data Analysis\n",
    "For datasets containing text data, such as tweets, conduct text-specific analyses:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c789b50",
   "metadata": {},
   "source": [
    "#### Word Frequency Count:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c829bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import explode, split\n",
    "tweets_df.withColumn(\"word\", explode(split(tweets_df[\"text\"], \"\\s+\"))).groupBy(\"word\").count().orderBy(\"count\", ascending=False).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28dd17e",
   "metadata": {},
   "source": [
    "#### Word Cloud:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20b4b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "text_data = tweets_df.select(\"text\").rdd.flatMap(lambda x: x).collect()\n",
    "text_string = \" \".join(text_data)\n",
    "wordcloud = WordCloud().generate(text_string)\n",
    "\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30cca4b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "145b6685",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4559b90",
   "metadata": {},
   "source": [
    "Prepare the data for analysis, which may include parsing dates or extracting specific features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411d965e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_date\n",
    "\n",
    "# Convert 'date' to a proper date format if needed\n",
    "clean_tweets_df = clean_tweets_df.withColumn(\"date\", to_date(\"date\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0302f912",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1fa56435",
   "metadata": {},
   "source": [
    "### Sentiment Analysis \n",
    "\n",
    "Choose Sentiment Analysis Technique:\n",
    "Depending on the libraries available, setup for sentiment analysis might involve using an NLP library or building a custom model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a3bc28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example using TextBlob for sentiment analysis (simplistic and illustrative)\n",
    "from textblob import TextBlob\n",
    "\n",
    "def sentiment_analysis(text):\n",
    "    return TextBlob(text).sentiment.polarity\n",
    "\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import FloatType\n",
    "\n",
    "sentiment_udf = udf(sentiment_analysis, FloatType())\n",
    "\n",
    "# Apply sentiment analysis\n",
    "sentiment_df = clean_tweets_df.withColumn(\"sentiment\", sentiment_udf(clean_tweets_df[\"text\"]))\n",
    "sentiment_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35906e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bd9baef7",
   "metadata": {},
   "source": [
    "### Time Series Analysis and Forecasting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0309d910",
   "metadata": {},
   "source": [
    "#### Aggregate Data for Time Series:\n",
    "Prepare the data for time series analysis, typically needing aggregation by date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4c6c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import avg\n",
    "\n",
    "daily_sentiment = sentiment_df.groupBy(\"date\").agg(avg(\"sentiment\").alias(\"avg_sentiment\"))\n",
    "daily_sentiment.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34785da3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f489ff79",
   "metadata": {},
   "source": [
    "#### Forecasting:\n",
    "Implement time series forecasting models such as ARIMA or LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe08b70",
   "metadata": {},
   "source": [
    "#### Visualization and Reporting\n",
    "Dynamic Dashboard Creation:\n",
    "Use an appropriate tool like Plotly or PowerBI to create an interactive dashboard. Prepare the data in notebook and export it for visualization.\n",
    "\n",
    "Documentation:\n",
    "Document the findings, methods, and justifications in the report according to the project guidelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b4cd7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b15b1387",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17bb926b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa280624",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ce7d8fa7",
   "metadata": {},
   "source": [
    "References:\n",
    "\n",
    "https://spark.apache.org/docs/latest/api/python/development/debugging.html#:~:text=1%202%204-,Py4j,its%20stack%20trace%2C%20as%20java.\n",
    "\n",
    "Apache Parquet Documentation\n",
    "The official documentation for the Parquet file format offers insights into its design, features, and benefits for using it in data storage and processing tasks.https://spark.apache.org/docs/latest/\n",
    "\n",
    "Databricks Resources\n",
    "Databricks, a company founded by the creators of Apache Spark, provides extensive resources, blogs, and tutorials on Spark and Parquet, including best practices for performance optimization.\n",
    "Link: Databricks - Apache Spark Resources\n",
    "\n",
    "This book by Holden Karau, Andy Konwinski, Patrick Wendell, and Matei Zaharia (O'Reilly Media) is a great resource to learn about Spark from the ground up, covering basic to advanced topics.\n",
    "ISBN: 978-1449358624"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a0c256",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
